% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

%\documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}
\usepackage{color}
\usepackage{multirow}
\usepackage{listings}
\usepackage{url}
\usepackage{listings}
\lstset{language=sql, escapeinside={\$}, basicstyle=\footnotesize\ttfamily, mathescape, escapeinside={\%*}{*)}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]

\newif\ifdraft
\drafttrue
%\draftfalse                                                                              
\ifdraft
\newcommand{\zhaonote}[1]{{\textcolor{cyan}    { ***Zhao:      #1 }}}
\newcommand{\evannote}[1]{{\textcolor{red}    { ***Evan:      #1 }}}
\newcommand{\note}[1]{ {\textcolor{red}    {\bf #1 }}}
\else
\newcommand{\zhaonote}[1]{}
\newcommand{\evannote}[1]{}
\newcommand{\note}[1]{}
\fi

\newenvironment{shortlist}{
        \vspace*{-0.5em}
  \begin{itemize}
  \setlength{\itemsep}{-0.1em}
}{
  \end{itemize}
        \vspace*{-0.5em}
}

\begin{document}

\title{SystemL: Diagnosing Machine Learning Pipelines with Fine-grained Lineage}

%\numberofauthors{1} 
%\author{
%\alignauthor Zhao~Zhang, Evan~R.~Sparks, Michael~J.~Franklin \\\
%       \affaddr{AMPLab, University of California, Berkeley}\\
%       \email{\{zhaozhang, sparks, franklin\}@cs.berkeley.edu} 
%}

\maketitle

\begin{abstract}
We present the SystemL lineage system to enable the diagnosis of distributed machine learning (ML) pipelines by leveraging the fine-grained
data lineage.  
SystemL exposes a concise yet powerful API, derived from \emph{primitive lineage types}, and collects fine-grained data lineage for each 
data transformation by recording the input datasets, the output datasets and the mapping between them, along
with all information needed to reproduce the computation.
SystemL efficiently enables data anomaly removal and computation replay, code debugging, and result analysis.
For a single transformation, SystemL's high-order function mapping strategy can reduce the storage consumption by up to 
45x comparing to the element-wise mapping recording.
The spatial index allows SystemL to speed up query performance by a further 2x-31x. 
The performance penalty introduced by SystemL is 16\%-54\% depending on the pipeline and use case.
\end{abstract}

% A category with the (minimum) three required fields
\category{Data management systems}{Database design and models}{Data model extensions}[Data provenance]
%\keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
Machine learning frameworks are increasingly popular as practitioners and researchers can quickly
build applications (referred to as ML pipelines in the rest of this paper or pipelines in the rest of this paper) with a high-level 
programming language to pipeline data preparation, feature extraction, model training, and prediction. 
Among these systems, Scikit-learn~\cite{pedregosa2011scikit}
is a single-computer-based framework while TensorFlow~\cite{tensorflow15}, Apache Mahout~\cite{owen2011mahout}, MLlib~\cite{meng2015mllib}, 
SystemML~\cite{ghoting11systemml}, and KeystoneML~\cite{sparks15} focus on a distributed environment.
In practice, to obtain a working model, users often need to try a number of options (e.g. featurization, training parameters, and datasets). 
When experimenting with these options, users frequently analyze results together with the corresponding input data,  
locate the code segment for unexpected outcome, and rerun the computation with the removal of suspicious data anomalies
identified as bad by domain experts due to measurement error.
Commonly, they use a case-by-case solution to record necessary information and store such information in persistent storage for examination.
These solutions are often use case specific, performance degradation prone, and they can complicate code
management due to the demand for multiple versions of a pipeline.
Given that this is a repeated pattern taken on by users of the system, it is natural to add support 
that is flexible for users in terms of lineage specification, instrumentation and query.

In this paper, we describe the design of a diagnostic system for distributed ML frameworks by leveraging the fine-grained lineage information
(data lineage at a structure cell level, e.g. elements in a matrix). 
We assume an ML framework with a programming model that abstracts a pipeline as a chain of data transformations, such as KeystoneML
which builds on top of Apache Spark, and use two building blocks: transformers and estimators.
A transformer applies a deterministic unary function to data items and produce new data items.
An estimator takes a collection of data items, and feed them to a training procedure,  and produces a transformer.
We choose this model because users can declare and instrument lineage collection in the transformer.
This diagnostic system enables users to specify the data transformation lineage with a concise interface, customize the instrumentation, 
collect fine-grained lineage with a minimal performance penalty, and serve the input/output dependency query with a low latency. 

%answer sophisticated lineage query responsively( with query latency lower than 10~seconds~\cite{nielsen2009}).
Lineage tracking has been investigated in various areas.
Existing lineage tracking system such as Chimera~\cite{foster02} and and Taverna~\cite{oinn02} can collect coarse-grained lineage 
of files and computations for scientific workflow systems.
The weak inversion and verification methods~\cite{woodruff97} for RDBMS-based visualization system 
was proposed for queries that can tolerate inaccurate fine-grained lineage information.
Data warehouse fine-grained lineage capturing and serving is enabled by works~\cite{cui00, cui03}.
Trio~\cite{widom04} tracks lineage for the RDBMS with cell mappings materialization at the same time of view queries.
Works in fine-grained lineage capturing such as GMRW~\cite{ikeda11} and Newt~\cite{logothetis13} presents
solutions for MapReduce systems, and capture lineage on low level operators such as mappers and reducers.
SubZero~\cite{wu13}, which is built on SciDB~\cite{brown10} has a similar lineage capturing interface at operators and UDFs.

Unfortunately, none of the existing lineage tracking systems is suitable for the distributed ML frameworks for three reasons.
First, the lineage information collected by scientific workflow systems (e.g., Chimera) lacks the details for cell-level lineage tracing and query.
Second, the weak inversion and verification methods relax lineage accuracy thus can not provide accurate lineage information for ML pipeline diagnosis.
Third, the lineage capturing interface of Trio, GMRW, Newt, and SubZero is designed by at
the underlying system operators (e.g., SQL operators for RDMBS, aggregate-select-project-join operators in data warehouse, 
mappers and reducers in MapReduce systems, and the pre-defined operators and UDFs in SciDB). 
This approach is general for all workloads running on the system and completely transparent to users, 
thus it gives users very limited flexibility for customized instrumentation (specification of lineage type and the data to be collected) 
in the case when users know the exact data to collect.

In this work, we present a lineage capturing interface that returns declaration and instrumentation flexibility to users. 
By formulating the cell level mapping of an ML pipeline as a sequence of multi-dimensional space transformations
(Figure~\ref{fig:conceptual} shows an conceptual illustration), we are able to identify seven primitive lineage types.
Higher-dimensional space can collapse/expand to lower-dimensional spaces along one dimension at a time, 
resulting in {\bf collapse} mapping(\textcircled{1}) and {\bf expansion} mapping(\textcircled{6}, \textcircled{7}). 
Mappings between spaces with the same number of dimensions can be {\bf identity} mapping(\textcircled{5}), {\bf all} mapping(\textcircled{9}), 
{\bf geometry} mapping(\textcircled{2}, \textcircled{4}), {\bf lincom} (short for linear combination) mapping(\textcircled{3}, \textcircled{8}),
and {\bf join} mapping. 
This interface covers 87\% (39 out of 45) of the data transformations in the current code base of KeystoneML. 
For transformations that are not covered, the lineage capturing interface allows users to specify a customized mapping function.
A formal definition of the mapping types is in \S\ref{sec:Design-Mapping}.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/Conceptual}
    \caption {A synthetic image classification pipeline with elements mapping highlighted in color scheme. 
    \textcircled{1}: Conversion of multi-channel image to gray scale.
    \textcircled{2}: Local feature extraction.
    \textcircled{3}: Vector dimension reduction.
    \textcircled{4}: Vector sampling.
    \textcircled{5}: Conversion from float to double.
    \textcircled{6}: Vector combining.
    \textcircled{7}: Matrix vectorizing.
    \textcircled{8}: Linear model prediction.
    \textcircled{9}: Selection of the index with maximum value.
    \label{fig:conceptual}
}
\end{center}
\end{figure}


We present SystemL, a lineage collecting and serving system that implements the above general solutions.
SystemL is integrated with the KeystoneML~\cite{sparks15} distributed machine learning framework.
SystemL interacts with KeystoneML at the transformer level, where the users can declare the lineage type and instrumentation.
SystemL exploits the  data structure metadata (e.g., the size of each dimension of a space) for mapping recording to reduce storage overhead.
It also uses a high-order function approach and spatial index for geometry mapping which does not have a regular mapping pattern.
SystemL collects five types of data for each transformer: the input dataset, the output dataset, the mapping between them
along with the computation and the optional model.
SystemL stores the lineage information on HDFS~\cite{shvachko10} via the resilient distributed dataset (RDD) abstraction
of Apache Spark~\cite{zaharia12}, the underlying computing engine of KeystoneML.
Unlike Spark lineage, which tracks partition level data dependency and associated computation for internal system resilience,
SystemL tracks lineage with a finer granularity at the cell level of data structures.
In addition, SystemL exposes an interface for users to specify lineage types and query.
When queried, SystemL can load the lineage information from storage and reconstruct the lineage chain.
Users have the flexibility to reconstruct a single transformer lineage, a partial chain, or the whole chain.


Combining the high-order function approach and spatial indexing strategies, 
the geometry lineage query is improved by 2x-31x.
Storage consumption is reduced by up to 45x compared to the element-wise mapping recording. 
Our measurements with the three representative ML pipelines show that SystemL lineage 
capturing scales no worse than the ML pipelines.
The performance penalty for three typical use cases (discussed in \S\ref{sec:Back-cases}) ranges from 16\% to 54\%.
The query for these use cases takes from 0.1~s to 1.8~s. 

In summary, we make the following contributions:
\begin{shortlist}
\item{} We present a concise and powerful lineage system interface by formulating the cell-wise mapping in ML pipelines as multi-dimensional space transformation.
\item{} We present a high-order function to describe the geometry to reduce storage overhead.
\item{} We present and evaluate various spatial indexing strategies to improve the query performance.
\item{} We present three real use cases of using fine-grained data lineage for code validation, results inspection, and data cleaning, respectively.
\end{shortlist}


\section{Use Cases and KeystoneML}
\label{sec:Background}
In this section, we present three motivating use cases of SystemL and briefly review KeystoneML.

\subsection{Use Cases}
\label{sec:Back-cases}
We select three use cases from real ML pipelines, these three cases are representative for code validation, results inspection, and data cleaning, respectively.

\subsubsection{Code Validation}
In this use case we use lineage as a tool in the debugging process as pipelines change and are augmented. 
Typical machine learning workflows are constructed via an iterative proces of refinement with a machine learning developer or data scientist in the loop.
These users are constantly engineering new features, adding new datasources, and trying out new machine learning methods at all stages of the pipeline.
Lineage offers a natural way for these users to hone in on the areas where two similar pipelines diverge in terms of their intermediate data, and presents a new avenue for investigation of model performance by allowing users to pinpoint the point at which their data diverged from a known good pipeline and to inspect \emph{what} changed during the data processing procedure. 


\begin{figure}[ht]
\begin{center}
    \includegraphics[width=85mm]{pictures/VOCSIFTFisher}
    \caption {The SIFTFisher pipeline, boxes are dataset, rounded-corner boxes are transformers, dashed rounded-corner boxes are estimators, shaded boxes are calling out to C library for computation.
    \label{fig:vocsiftfisher}
}
\end{center}
\end{figure}

As a concrete example, consider the SIFTFisher pipeline, shown as Figure~\ref{fig:vocsiftfisher}. 
It has been shown that augmenting traditional SIFT~\cite{lowe99} (Scale-invariant feature transform) descriptors with additional location information about the location of the descriptor in the input image can improve classification performance.
However, when we first developed such a pipeline, our results conflicted with published work indicating that these features provided a statistically significant improvement in classification accuracy.
By employing lineage based debugging, we could isolate exactly where in the pipeline our calculations diverged from the original features, and diagnose impacts seen downstream like a under-fit GMM (Gaussian Mixture Model), and trace the ultimate lack of classification improvement to a bug in the underlying C library which we called into.
By visualizing the features we could see that the new features added to our model did not correspond to positions in the image, but rather contained values outside of a suitable range for these features, which turned out to be randomly allocated memory. The SIFT feature visualization involves backward query on the lineage of the SIFTExtractor transformer and highlights of the corresponding input pixels.


\subsubsection{Results Inspection}
Users of ML pipelines often interpret the results, identify some potential data anomalies, then retrospect the supporting input data for better understanding.
Fine-grained data lineage of ML pipelines have sufficient information for such investigation.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/SourceExtractor}
    \caption {The SourceExtractor pipeline, boxes are dataset, rounded-corner boxes are transformers, dashed rounded-corner boxes are estimators, shaded boxes are calling out to C library for computation.
    \label{fig:sourceextractor}
}
\end{center}
\end{figure}

One such use case is results validation in the SourceExtractor pipeline, shown in Figure~\ref{fig:sourceextractor},
that processes images from the telescopes and produces astronomical object catalogs. 
The astronomical object catalog includes the position, shape, and other statistical properties of the object.
In this case, astronomers find the bright objects over a threshold and validate that these objects are indeed astronomical objects rather
than noise caused by cosmic rays.
With the lineage information, astronomers can first filter the results to find the objects whose brightness is above a threshold.
Then using backward query, they can find corresponding pixels for these bright objects. 
Either through mathematical analysis or human intervention, they are able to validate the correctness of the bright objects.

\subsubsection{Data Cleaning}
\label{sec:Back-Case-Cleaning}
Another use of the lineage information is for users to apply a subset of the prepared dataset to fit the model. 
A user of the SIFTFisher pipeline, shown in Figure~\ref{fig:vocsiftfisher}, observes a low prediction accuracy for a category of images, especially 
when cats and dogs are in the same image. A natural investigation is then to remove the images with both cats and dogs,
and test if that improves prediction accuracy.

Without lineage information, the user has to remove the according images from the training set and rerun data
preparation part (from PixelScaler to RowNormalizer), then feed the dataset to linear solver to train the model.
With lineage information, the user can intercept the pipeline by loading the output of RowNormalizer directly into KeystoneML,
filter out the data items that are derived from the images that have people and dogs together , and apply the filtered dataset directly to the linear solver.
In this case, the rerunning process can bypass the expensive data preparation procedure resulting in shorter turn-around time.

\subsection{KeystoneML}
KeystoneML is an application framework designed for the implementation of robust large-scale machine learning pipelines. Built on the principles of declarative programming and modular design, KeystoneML provides a light-weight and elegant API that allows users to describe these pipelines as the composition of two types of operator--transformers, which perform deterministic data transformation, and estimators, which ``learn'' transformers based on training data. KeystoneML pipelines are fit and executed in parallel using Apache Spark. These pipelines are compiled into an application DAG and optimized before execution. Current optimizations include online decisions about materialization of intermediate state, as well as standard optimizations such as common subexpression elimination. KeystoneML includes a library of standard feature extractors in domains including computer vision, audio, and text processing, as well as standard statistical procedures and Estimators for several types of machine learning model.

\section{Formal Discussion}
\label{sec:Mapping}
In this section, we formally define the notion of ML pipeline, dataset, mapping and its associated operations of query in the context of 
relational database.

\subsection{Pipeline and Datasets}
\label{sec:Map-Pipe-Data}
We define a dataset $I$ as a collection of structured data. In the scope of this work, the supported structures are: 
string, vector, matrix, and image. A vector or a string is a one dimensional data structure and a matrix is two dimensional. 
For simplicity, we view string as a vector.
An image is a three dimensional data structure that is defined by the height, the width and the number
of channels. 

An element $e \in I$ has two properties: value $e.value$ and coordinate $e.coor$ (referred as metadata or meta
in the following discussion). 
Combining the additional dimension in the collection (RDD or sequence) with the data structure, 
the metadata of an element is a pair of integers, a triple of integers, or a quadruple of integers
for vector, matrix, and image, respectively.

A pipeline $P$ is defined as a sequence of transformers $(T_1, T_2, ..., T_n)$. 
Each transformer $T_i$ takes a dataset $I_i$ as its input, and produces $I_{i+1}$ as the output: 
$I_{i+1} = T_i(I_i)$. 
Especially, we denote the output of the whole pipeline as $O = I_{n+1}$

To simplify the discussion, we use the vector data structure as example in the following discussion.
Other data structures can be defined in a similar way. 
For queries, we present the definition of forward query and backward query can be inducted by reversing
the table order in the join operation.

\subsection{Dataset and Mapping}
\label{sec:formal-ds-mapping}
Conceptually, the input dataset and output dataset can be defined
as tables with each element's position in the data structure as key.
In reality, such tables is in the form of a distributed collection such as RDD.
\newline\newline
\noindent\begin{minipage}{\textwidth}
\noindent\begin{minipage}{.2\textwidth}
\begin{verbatim}
CREATE TABLE input
(
  Meta (INT, INT),
  Value (DOUBLE)
);
\end{verbatim}
\end{minipage} 
\qquad{\color{black}\vrule}\qquad
\begin{minipage}{.2\textwidth}
\begin{verbatim}
CREATE TABLE output
(
  Meta (INT, INT),
  Value (DOUBLE)
);
\end{verbatim}
\end{minipage}
\end{minipage}
\vspace{2ex}

The mapping between the input and output dataset is a left outer join of the the two tables with a mapping function.
The Fun function returns the corresponding output elements given an input element. 
It is a left outer join since an input element may not be corresponding to any output element.
\begin{lstlisting}
SELECT *
From input
LEFT OUTER JOIN output 
    ON Fun(in.meta) $\ni$ out.meta;
\end{lstlisting}

In practice, we separate the metadata from the actual element values and only record metadata mapping.
\begin{lstlisting}
INSERT INTO mapping (in_meta, out_meta)
SELECT in.meta, out.meta
From input
LEFT OUTER JOIN output 
    ON Fun(in.meta) $\ni$ out.meta;
\end{lstlisting}
The retrieval of actual element values can be obtained by joining the mapping table and the output dataset table.
\begin{lstlisting}
SELECT mapping.meta, out.value
From mapping
LEFT OUTER JOIN output 
    ON mapping.out_meta = output.meta;
\end{lstlisting}

The input and output mapping of a pipeline is in turn a sequence of mappings of individual transformer.
Consider a pipeline $P=(T_1, T_2, ..., T_n)$, we denote $mapping_i$ as the mapping for transformer $T_i$.
The mapping of the pipeline is a sequence of left outer join operations of $\{mapping_1, ..., mapping_n\}$ on 
$mapping_i.out\_meta = mapping_{i+1}.in\_meta$.

\subsection{Mapping Query}
A forward query with a key  ({\it qForward(key)}) over the mapping of a transformer or a pipeline can be expressed as:
\begin{lstlisting}
SELECT out_meta
FROM mapping
WHERE key = in_meta;
\end{lstlisting}

A forward query with a list of $keys={k_1, k_2, ..., k_n}$ ({\it qForward(keys)}) can be expressed as:
\begin{lstlisting}
SELECT out_meta
FROM mapping
WHERE $k_1$ = in_meta OR ... OR $k_n$=in_meta;
\end{lstlisting}

\subsection{Data Preparation Skipping}
\label{sec:formal-skipping}
In use cases such as the data cleaning presented in \S\ref{sec:Back-Case-Cleaning}, users want
to remove certain suspicious input data items that might relate to bad prediction results and retrain the model then
test its performance. 
Repeating the data preparation steps (e.g., from PixelScaler to RowNormalizer in SIFTFisher pipeline) incurs
long running time.
One natural thought in such case is ``can we skip the computation applied on the unchanged data items by removing
the corresponding prepared data items directly?''

Formally, given a data transformation {\it f()} applied on a collection $C$ of vectors \{$v_1, v_2, ..., v_n$\}.
we say it is safe to remove {\it f($v_i$)} when {\it $f(C/v_i)) = f(C)/f(v_i))$}.
The notion {\it $C/v_i$} can be interpreted as the collection derived by removing $v_i$ from $C$.
In other words, function {\it f()} is distributive on vectors \{$v_1, v_2, ..., v_n$\}. 
With single or few transformers, users are able to track the distributive property.
However when the pipeline is deep, such property is hard to track.

Using the fine-grained data lineage, we present a validation method for the distributive property.
We say the mapping of {\it f()} is self-reflected if $qBackward_f(qForward_f(v_i)) \subseteq v_i$, where vector $v_i$ can be viewed 
as a set of one-dimensional elements.


\begin{theorem}
\label{thm:distributive}
A data transformation f() is distributive if the mapping of f() is self-reflected.
\end{theorem}

\begin{proof}
By contradiction, if f() is not distributive on collection $C$ of vectors  \{$v_1, v_2, ..., v_n$\}, then $\exists e \in v_j $ s.t. $f(v_i)$ takes as input.
Then $e \in qBackward_f(qForward_f(v_i))$ and $e \in v_i$ which results in  $qBackward_f(qForward_f(v_i)) \nsubseteq v_i$, which violates condition.
\end{proof}

\begin{corollary}
A sequence of data transformations ($s = {f_1, f_2, ..., f_n}$) is distributive if the mapping between the very input and output datasets is self-reflected.
\end{corollary}

\subsection{Mapping Function}
\label{sec:Map-Func}
The mapping function {\bf Func} contains the core information of the input and output element dependency.
Recording the element-wise dependency can take up to $O(kN^2)$ space given k N-dimensional vectors in the case
of every output element depends on every input element. 
To eliminate this polynomial explosion in space requirements, we identify seven primitive mapping functions.
From higher dimension space to lower dimension space, the mapping can be {\bf collapse} or {\bf expansion}.
Between the equal dimension spaces, the mapping can be {\bf identity}, {\bf all}, {\bf geometry}, {\bf lincom}, and {\bf join}.
Figure~\ref{fig:narrowmapping} shows the seven primitive mapping functions, with the mapping highlighted with color scheme.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/narrowmapping}
\caption {Seven primitive mapping functions.
    \label{fig:narrowmapping}
}
\end{center}
\end{figure}


{\bf Collapse} mapping describes the mapping of a space collapsing into a one dimension lower space along one dimension which needs
to be specified by the user. Examples include gray scaling a multiple channel image.
{\bf Expansion} mapping is where a space expands to a one dimension lower space, e.g. expanding a matrix to a vector with column major.
In {\bf Identity} mapping, each output element only depends on one element with the same coordinate in the input matrix.
In {\bf All} mapping, each output element depends on all elements in the input matrix.
{\bf Geometry} mapping describes the case where a group of output elements depend on a group of input elements.
{\bf LinCom} mapping describes the data dependencies in a linear model.
{\bf Join} mapping happens between equal dimension space transformation, where nested collection of data structure is reorganized by 
transposing two dimensions
For the mappings that are not covered by the primitive functions, we allow users to define their own mapping functions.

%Since transformers operate at the data structure (vector, matrix, image) level, we use the notation
%$\overline{I_i(e)}$ as the companion of $I_i$ with respect to element $e$ such that $\forall a \in \overline{I_i(e)}$,
%\[ a.value =
%  \begin{cases}
%    e.value       & \quad \text{if } a.coor = e.coor\\
%    NaN  & \quad \text{otherwise}. \\
%  \end{cases}
%\]

%Accordingly, given a list of distinct elements $(e_1, ..., e_m)$, the companion of $I_i$ with respect to $(e_1, ..., e_m)$ is
%described as: $\forall a \in \overline{I_i(e_1, ..., e_m)}$, 
%\[ a.value =
%  \begin{cases}
%    e.value       & \quad \text{if } \exists e \in (e_1,...,e_m), a.coor = e.coor\\
%    NaN  & \quad \text{otherwise}. \\
%  \end{cases}
%\]

%Before defining the lineage, we need to introduce the relationship between the two data structures $A$ and $B$ with the same type.
%We say $A \subseteq B \text{ or } B \supseteq A$, $\text{if }\forall a \in A, \exists b \in B, s.t.\text{ } a \neq NaN \land a.coor = b.coor \land a.value = b.value$.
%By saying $A = B$, we mean $A \subseteq B$ and $B \subseteq A$.
%
%We define the data lineage of a given element as the elements in the input dataset that is used to produce present element.
%The single transformer lineage is
%\begin{equation}
%S(e \in I_{i+1}) = \{e_1, ..., e_m | T_i(\overline{I_i(e_1, ..., e_m)}) \supseteq \overline{I_{i+1}(e)}\}.
%\label{equa:SingleLineage}
%\end{equation}
%
%Recursively, the lineage of a given element across multiple transformers can be defined as
%\begin{equation}
%L(e) = \cup_{e' \in S(e)} L(e').
%\end{equation}
%
%We define the property 
%\begin{equation}
%Replay(e \in I_{i+1}) = \overline{I_{i+1}(e)} \subseteq T_i(\overline{I_i(e_1, ..., e_m)})
%\end{equation}
%as the replay property, which means we can reproduce an output element $e$ by applying the corresponding transformer to its data lineage.
%The constraint of the replay property is that only the element $e$ is guaranteed to be the same as the original transformation,
%if $\overline{I_{i+1}(e)} \subset T_i(\overline{I_i(e_1, ..., e_m)})$. 
%In this case, the results of the replayed transformer may produce inconsistent results for elements other than $e$ in $I_{i+1}$. 
%Similarly, the replay property for multiple elements only guarantees the correctness of these elements themselves.
%
%\subsection{Data Lineage Query}
%The forward query of an element over data lineage of a pipeline $P$ returns $qForward(e) = \{e' | e \in L(e' \in O)\}$. 
%And the backward query of an element over data lineage returns $qBackward(e) = e' \in L(e)$.
%Specifically, the identifier of an element is its coordinates in the according data structure, so the query actually takes
%the coordinates of the input element as key, and query over the data lineage. The query that spans multiple transformers 
%use coordinates as intermediate data. Once the query reaches the final transformer, it can associate the actual values
%with the result element coordinates and return them.
%
%Queries of multiple elements can then be expressed as the union of the query for each element: 
%\begin{equation}
%\begin{split}
%qForward(e_1, ..., e_m) = \cup_{e \in (e_1, ..., e_m)} qForward(e) \\
%qBackward(e_1, ..., e_m) = \cup_{e \in (e_1, ..., e_m)} qBackward(e).
%\end{split}
%\end{equation}

\section{Design Goals}
\label{sec:Req}
In this section, we present the design goals of SystemL from the functional and performance perspective.

\subsection{Functional Requirements}
Two basic functions of SystemL is lineage capturing and serving.
SystemL requires a programming interface between the users of KeystoneML and the lineage capturing unit to 
collect lineage information and customize instrumentation (deciding what dataset to store). 
This interface should be concise with a few variations and options.
This interface should also be powerful in terms of coverage of transformers.
This interface should be flexible so that users can define their own lineage and instrumentation.

To serve lineage for query, SystemL requires a user interface to reconstruct and chain collected lineage.
Each transformer should provide the forward query (qForward()) and backward query (qBackward()) functionality.
SystemL needs to verify the chained lineage has identical ordering to the pipeline.
The single transformer lineage should be able to be replayed deterministically, meaning that with the same input dataset and transformer,
the lineage contains enough information with which users can reproduce the results.
In turn, a partial pipeline assembled by a sequence of transformers of the original pipeline should be able to be replayed deterministically.

\subsection{Performance Requirements}
\label{sec:Req-Perf}
In general, as lineage capturing incurs additional computation and I/O for mapping materialization and storage, a non-trivial performance
penalty based on the original workload is expected. 
Table~\ref{tb:overhead} summarizes documented performance penalty introduced by lineage capturing in recent works. 

\begin{table}[ht]
\begin{center}
    \caption{Performance Penalty Reported in Recent Lineage System}
    \begin{scriptsize}
    \begin{tabular}{ | p{2cm} | p{2.5cm} | p{2.8cm} |}
    \hline
    Project & Underlying System & Performance Penalty  \\ \hline \hline
    GMRW~\cite{ikeda11} & Hadoop~\cite{HADOOP} & 16-76\% \\ \hline
    Newt~\cite{logothetis13} & Hadoop~\cite{HADOOP} & 20-50\% \\ \hline
    SubZero~\cite{wu13} & SciDB~\cite{brown10} & 50\%-150\% \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:overhead}
\end{center}   
\end{table}

This performance penalty problem exacerbates for SystemL, since KeystoneML's distributed computing engine Apache Spark 
optimizes for in-memory computing and do not write intermediate data to disks unless it is a shuffle operation. 
Comparing to systems such as Hadoop, the same amount of I/O of SystemL will introduce a more significant penalty.
Despite this obstacle, we expect SystemL's performance penalty to be comparable to the existing systems.

Lineage information can be massive depending on the pipeline, since the input or the intermediate dataset of a pipeline can be large.
In some transformers, the mapping between the input elements and output elements does not conform a pattern, e.g., Geometry mapping.
We need advance technique to reduce the storage burst introduced by such mappings.

Using SystemL for ML pipeline diagnosing requires a low-latency interaction between users and the lineage serving unit.
As documented in the report~\cite{nielsen2009} in human computer interaction, 1~second latency is the boundary to keep users' thought flow while 10~seconds
is the boundary to loose users' attention.
We expect SystemL's query latency within the 10~seconds boundary to facilitate users diagnosis on ML pipelines.

\section{Design}
\label{sec:Design}
In this section, we discuss SystemL's layered design of lineage capturing and instrumentation interface, 

\subsection{Overview}
Figure~\ref{fig:architecture} shows the overview of SystemL and its interactions with other components. 
Rounded-corner rectangles are components around and inside KeystoneML. 
The two-sided arrows indicate interactions between components.
Users compose machine learning pipelines with KeystoneML and submit the compiled DAG to Spark.
All computation and I/O are done through the Spark Resilient Distributed Dataset (RDD) abstraction.
With SystemL, users can declare and instrument lineage at the transformer level, 
so that the compiled DAG also contains lineage recording and other operations, e.g., save to disk.
Lineage in SystemL is stored to HDFS~\cite{shvachko10} via Spark's RDD abstraction.
Users can load the stored lineage in HDFS to Spark interactive command line interface.
By interacting using the query interface, users can query elements of interest, replay transformation, or analyze
the lineage for other purposes.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/architecture}
\caption {Overview of SystemL and its surrounding components.
    \label{fig:architecture}
}
\end{center}
\end{figure}

\subsection{Lineage Interface}
\label{sec:Design-Lineage}
SystemL exposes the lineage capturing interface by naming the lineage type specific class with its mapping types between input and output datasets.
All the type specific class inherits an abstract class of Lineage, which define common methods shared among the type specific classes:
\begin{shortlist}
\item{} qForward(keys: List[Coor]): List[Coor], given a list of element metadata (coordinates) returns the forward query results on the metadata mapping of the lineage
\item{} qBackward(keys: List[Coor]): List[Coor], given a list of element metadata (coordinates) returns the backward query results on the metadata mapping of the lineage
\item{} saveMapping(): saves the collection of metadata mappings of a lineage to disks
\item{} saveInput(): saves the input dataset to disks
\item{} saveOutput(): saves the output dataset to disks.
\end{shortlist}


Table~\ref{tb:lineage-interface} summarizes the lineage types and parameters passing by the users.
\begin{table}[ht]
\begin{center}
    \caption{Primitive Lineage Types}
    \begin{scriptsize}
    \begin{tabular}{ | p{8cm}|}
    \hline
    CollapseLineage(in, out, transformer, dimension) \\ \hline 
    ExpansionLineage(in, out, transformer, dimension) \\ \hline
    IdentityLineage(in, out, transformer) \\ \hline
    AllLineage(in, out, transformer) \\ \hline
    GeoLineage(in, out, transformer, mappingCollection) \\ \hline
    LinComLineage(in, out, transformer, model) \\ \hline
    JoinLineage(in, out, transformer, dimension) \\ \hline
    CustomLineage(in, out, transformer, fFunc, bFunc, model, rand) \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:lineage-interface}
\end{center}   
\end{table}

In general, SystemL lineage capturing interface can collect six types of data: 
the input dataset (in), the output dataset (out), the transformer (transformer), 
the collection of mapping (mappingCollection), the model (model), and the random 
parameter (rand).

By checking the input and output dataset, SystemL is able to figure out the metadata of each data structure then use
it to declare the mapping objects, which is described in the following section.

\subsection{Mapping}
\label{sec:Design-Mapping}
Mapping on the abstract level provides two interfaces: qForward() and qBackward(). Both interfaces
take a list of element metadata (coordinates) and return the query results accordingly.

For all mapping types presented in ~\S\ref{sec:Map-Func} except Geometry mapping, 
SystemL infers metadata of data structures automatically. 
For example in an All mapping between two matrices of 5 rows and 5 columns,
the mapping is recorded as AllMapping(twoDMeta(5, 5), twoDMeta(5,5)). 
So upon forward query, AllMapping returns the coordinates of all 25 elements in the output matrix by only looking into the matrix metadata.

We define three types of metadata to simplify the interface:
\begin{shortlist}
\item{} oneDMeta: Int
\item{} twoDMeta: (Int, Int)
\item{} threeDMeta: (Int, Int, Int)
\end{shortlist}  
SystemL also uses this data structure metadata to checkout out-of-bound errors if the given key is outside the space of the data structure.

For geometry mapping, the mapping is arbitrary and can not take benefit from data structure metadata, 
Intuitively, users to pass the exact mapping in the form of a collection, inside which is a list of tuples, where each tuple 
contains one set of input element coordinates and one set of output geometry element coordinates. 
This simple solution takes a tremendous amount of space to materialize the mapping in memory and store on disks.
The next section discusses the technical approach to reduce the size of geometry mapping and according indexing strategies.


\subsection{Geometry Mapping}
\label{sec:Design-GeometryMapping}
As can be seen in the geometry mapping example in Figure~\ref{fig:narrowmapping}, the backward query of an output element should return all the input elements in the corresponding geometry. If the output geometry has $N$ elements and the input geometry has $N$ elements, the space complexity of storing such a mapping is $O(N^2)$. 
Inspired by the Scale-invariant feature transform (SIFT) and the SourceExtractor implementation, each of them processes an input matrix and produces thousands of vectors as outputs. An output vector depends on a group of elements in the matrix, and these elements form a two dimensional shape. 
For SIFT, the input elements form a circle with a geographical center and the scale as its radius.
Similarly, the SourceExtractor's input elements form an ellipse. 
SystemL uses a high-order function to describe the geometry such as circle and ellipse. 
Thus the mapping between two geometries is expressed as a tuple of two higher-order functions.
The space complexity goes down to O(1).

Since many softwares such as SIFT and SourceExtractor already return the geographical information with the results.
SystemL exposes a simple interface for two dimensional shapes as special cases.
Users can pass in the geographical information directly to declare rectangle, circle, and ellipse.

Using the high-order function involves an issue for query. 
As both the input elements and output elements are encoded with a high-order function. 
And the mapping between input function and output function is stored in a list. 
A query that takes an output element as key thus needs to iterate over all input functions and test if the key is in the function or not.
If the key is in an input function, it then returns the mapping output function, and expands the function to a group of coordinates.
The time complexity of query over geometry mapping is then $O(N^2)$, assuming there are $N$ tuples in the list and each input function expands to $N$ elements.
To speedup the query performance, we implement and compare several indexing strategies, and a detailed discussion is in \S\ref{sec:GeometryIndex}.


\section{Implementations}
\label{sec:Impl}

This section discusses implementation details for lineage collection, I/O, and geometry mapping index.

\subsection{Metadata Separation}
Capturing lineage and storing it introduces performance penalty compared to the lineage-free case. 
One way to minimize this penalty is to store less data without sacrificing the integrity of the lineage information.
As discussed in \S\ref{sec:formal-ds-mapping}, SystemL separates the metadata of a data structure from the 
actual data (e.g., the coordinates of elements in a matrix), and mandates metadata mapping while leaves
the actual dataset storage an option for users.

Storing only the metadata mapping can still preserve the lineage information integrity given that the intermediate and result 
dataset can always be reproduced with the KeystoneML transformer given the its determinism property. 
A profile study on the SIFTFisher, SourceExtractor and MNIST pipelines shows a space reduction by 
13.7x (from 725.6~GB to 109.9~GB), 2480.9x (from 507.1G to 204.4M), and 150.8x (from 11.9~GB to 78.9~MB)

\subsection{Mapping Inversion}
When formulating the cell level mapping of a transformer as a multi-dimensional space transformation, we define
the mapping in the direction from input to output and name the according lineage type with its mapping type.
In this paragraph, we explain how to infer backward query (the mapping reverse) of the transformer.
{\bf All mapping}, {\bf identity mapping}, {\bf lincom mapping}, and {join mapping} are symmetric, 
meaning the reverse of these two mappings are also {\bf all mapping}, {\bf identity mapping}, 
{\bf lincom mapping}, and {\bf join mapping} respectively.

{\bf Collapse mapping} and {\bf expansion mapping} are not symmetric, but the mapping inversion can be inferred
given the data structure metadata (the number and size of dimensions of a n-dimensional space). For example, 
in the collapse case shown early in Figure~\ref{fig:narrowmapping}, a matrix with three rows and three columns
collapses into a vector of three along column dimension. Thus a mapping inversion of an element of of the vector
is all elements in the corresponding row in the matrix. 
The inversion of {\bf expansion mapping} can also be inferred with simple calculation.
The inversion of {\bf geometry mapping} is enabled by switch the order of the geometry order of each tuple.

For mappings that are not covered by the seven primitive types, we allow users to define {\bf CustomLineage}
which requires two functions, one for forward mapping and the other for backward mapping.

\subsection{Other Mapping Cases}
Among the seven mapping types defined in \S\ref{sec:Design-Lineage}, the {\bf collapse mapping} and {\bf expansion mapping}
map spaces that have different dimensions. By definition, they only maps from higher-dimensional space to lower-dimensional space.
A transformer that reduces lower-dimensional space to higher-dimensional space, e.g., transforming a vector into a matrix, can use
the same {\bf ExpansionLineage} type for declaration based on the fact that the reverse mapping of this transformation is a
{\bf expansion mapping}. SystemL can infer the mapping direction based on the metadata of data structures 
(the number and size of dimensions of a n-dimensional space). 
A similar solution is used for the reverse of {\bf collapse mapping}.

For sampling transformers, e.g., selecting a subset of a vector collection, SystemL implements it as a {\bf geometry mapping}.
From the input side, there are a list of geometries (rectangles for vectors) with each pointing to an output geometry (a rectangle for a vector).

In practice, we recommend users to define a transformer with one primitive mapping type to use SystemL and it is often the case since
we see 39 out of 45 transformers in the current KeystoneML code base are coded this way. 
In meantime, we also looking into ways to declare complex data transformations by composing the primitive mapping types.


\subsection{Lineage Storage}
SystemL uses the RDD abstraction of Apache Spark for lineage information storage. 
At a high level, SystemL creates a unique directory on the underlying file system with a naming pattern of 
concatenating the transformer name and the initial input dataset identifier of the pipeline.
In this way, SystemL can tell between the training pipeline and test pipeline based on the directory name
if the training and testing are run in a single program given that the training and test datasets are distinct.

For the datasets and mappings, SystemL write them to file system by calling {\it RDD.saveAsObjectFile(path)}.
For transformer, model and other parameters, SystemL wraps each of them into an RDD then store them as object files.


\subsection{Geometry Mapping Index}
\label{sec:GeometryIndex}
In \S\ref{sec:Design-GeometryMapping}, we introduced the notion of geometry mapping and how we use high-order function
to describe the geometry for less storage space. 
We also present that query of such geometry mapping can be slow due to the encoding. 
In this section, we discuss four indexing strategies to speedup the query over the geometry mapping. 
The four indexing strategies are referred as {\bf NoIndex}, {\bf Direct}, {\bf RTree}, and {\bf KMeans}, respectively.
For simplicity, we only discuss forward query from the input to the output in this section.
The backward query works in a similar way by reverting the the geometry tuple.


We use the example in Figure~\ref{fig:example} in the following discussion.
\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/example}
\caption {A geometry mapping example, the left matrix is input and the right matrix is output. The mapping between the geometries are shown using gray shades.
    \label{fig:example}
}
\end{center}
\end{figure}

Table~\ref{tb:example} shows the mapping for each element and the regressed 2D geometry mapping.
\begin{table}[ht]
\begin{center}
    \caption{Element-wise mapping and regressed geometry mapping of the example in Figure~\ref{fig:example}}
    \begin{scriptsize}
    \begin{tabular}{ | p{1.75cm} | p{1.75cm} | p{1.75cm} | p{1.75cm} |}
    \hline
    Input & Geometry & Output & Geometry \\ \hline \hline
    (0,0),...,(1,1) & Circle\_0 & (0,0),(1,0) & Rect\_0 \\ \hline
    (0,2),...,(1,3) & Circle\_1 & (0,1),(1,1) & Rect\_1 \\ \hline
    (2,0),...,(3,1) & Circle\_2 & (0,2),(1,2) & Rect\_2 \\ \hline
    (2,2),...,(3,3) & Circle\_3 & (0,3),(1,3) & Rect\_3 \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:example}
\end{center}   
\end{table} 

\subsubsection{No Index}
Using the example in Table~\ref{tb:example}, with {\bf NoIndex} strategy, query an element needs to traverse all circles (input geometries).
If the element is in a circle, the query returns the all output elements expanded from the mapping rectangle.
The time complexity of a query is $O(N)$, where N is the number of circles. 

\subsubsection{Direct Index}
To build the {\bf Direct} index, we use each input element as key and the associated geometry as the value, as shown in Figure~\ref{fig:direct}.
We rely on the underlying runtime system for the optimization of storing repeated geometries as an single object,
then using an pointer (hash value of the object) to point to the geometry object, as shown in Figure~\ref{fig:direct-optimized}.
A query of an element takes two steps: the first step returns the hash value for that element and the second step returns the rectangle. 
The space complexity of {\bf Direct} index is $O(k*N)$, where $k$ is the number of circles and $N$ is the number of elements in each circle.
For simplicity, the space complexity of the {\bf Direct} index is $O(N^2)$.
The time complexity of building index and query are $O(N^2)$ and $O(1)$, respectively.

\begin{figure}
\centering
\begin{minipage}{.3\linewidth}
  \includegraphics[width=\linewidth]{pictures/direct}
  \caption{Direct index}
  \label{fig:direct}
\end{minipage}
\hspace{.05\linewidth}
\begin{minipage}{.6\linewidth}
  \includegraphics[width=\linewidth]{pictures/direct-optimized}
  \caption{Direct index with optimization}
  \label{fig:direct-optimized}
\end{minipage}
\end{figure}


\subsubsection{RTree Index}
R-tree~\cite{guttman1984} has been introduced to index multidimensional information.
To simplify the discussion, we use the example shown in Figure~\ref{fig:example} and build the index in a two dimensional space.
The average query time complexity is $O(logN)$, while the worst case is $O(N)$. 
The worst case happens when querying an element in a geometry where all leaf nodes overlap.
Variants of R-tree, such as R+-tree~\cite{sellis1987} and R*-tree~\cite{beckmann1990}, 
seek to improve the worst case query by minimizing the overlapped geometry at leaf nodes 
by paying the cost for index building.

In our use cases, it is rare that a single element occurs in all geometries.  
R-tree is sufficient to validate the idea of using spatial indexing for geometry mapping.

We use an open source R-tree implementation, Archery~\cite{osheim13} to build the {\bf RTree} index.
In the Archery implementation, both the leaf nodes and the non-leaf nodes are abstracted as rectangles.
To differentiate from the rectangle geometries, we refer to it as box.
To adapt the geometry to Archery, first we compute the bounding box of each circle, then we use
a tuple of the bounding box and the circle as leaf node of the R-tree. 
The index is built upon the bounding boxes. 
The query of an input element (0,0) in the R-tree returns a tuple of the bounding box (center: (0.5,0.5), height:1, width:1) and circle\_0.
The query returns the output elements expanded from Rect\_0 only if the input element is within circle\_0.

The time complexity of building a R-tree is $log(N)$.
Practically the time complexity of query a R-tree is $log(N)$ in our use cases, though
the worst case complexity is $O(N)$.
And the space complexity for {\bf RTree} index is $log(N)$.

\subsubsection{KMeans Index}
We design a two-layer tree index with its leaf nodes as a list of geometries and its non-leaf nodes as the bounding box (same idea as that of R-tree)
of the geometry list.
The geometries are distributed to each non-leaf node using the KMeans~\cite{macqueen67} algorithm.

We specify $\sqrt{N}$ clusters, where N is the number of geometries, 
since $\sqrt{N}$ is the optimal cluster number for the worst case query when N geometries are uniformly distributed in a two dimensional space.
Clustering $N$ geometries into $k$ clusters results in $\frac{N}{k}$ elements in each cluster. 
Thus there are $k$ non-leaf nodes.
A worst case query needs to traverses all $k$ non-leaf nodes and all $\frac{N}{k}$ leaf node in the cluster.
$k+\frac{N}{k}$ has its minimum when $k=\sqrt{N}$. 

The time complexity of building the {\bf KMeans} index is $O(N^{1.5})$.
The average query time complexity is $O(logN)$.
The space complexity of {\bf KMeans} is $O(\sqrt{N})$.

\subsubsection{Index Strategy Asymptotical Comparison}
Table~\ref{tb:index-comparison} summarizes the index building time complexity, 
the query time complexity, and the index space complexity.

\begin{table}[ht]
\begin{center}
    \caption{Index Strategy Comparison}
    \begin{scriptsize}
    \begin{tabular}{ | p{1.75cm} | p{1.75cm} | p{1.75cm} | p{1.75cm} |}
    \hline
    Strategy & Building & Query & Space \\ \hline \hline
    NoIndex & 0 & $O(N)$ & 0 \\ \hline
    Direct & $O(N^2)$ & $O(1)$ & $O(N^2)$ \\ \hline
    RTree & $O(N)$ & $O(logN)$ & $O(logN)$ \\ \hline
    KMeans & $O(N^{1.5})$ & $O(logN)$ & $O(\sqrt{N})$ \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:index-comparison}
\end{center}   
\end{table}

{\bf Direct}'s query performance is the best from analysis, however the building time overhead and space overhead is quite high.
{\bf RTree} and {\bf KMeans} have a more balanced building time, space and query performance.
In reality, we see that the performance of {\bf RTree} an {\bf KMeans} are significantly affected by parameters such as the
number of boxes in each non-leaf node for {\bf RTree} and the number of iterations for {\bf KMeans}.
A detailed performance study is present in \S\ref{sec:Perf-Index}.

%\begin{table}[ht]
%\begin{center}
%    \caption{Index Strategy Profile on Single SIFTExtracror}
%    \begin{scriptsize}
%    \begin{tabular}{ | p{1.7cm} | p{0.9cm} | p{0.8cm} | p{0.9cm} | p{1cm} | p{1cm} |}
%    \hline
%    Strategy & NoIndex & Direct & RTree & KMeans1 & KMeans5 \\ \hline \hline
%    Average (ms) & 0.28 & 0.04 & 0.095 & 0.073 & 0.049 \\ \hline
%    Stdev & 0.08 & 0.011 & 0.022 & 0.011 & 0.014 \\ \hline
%    Space (MB) & 0.44 & 22.49 & 0.78 & 0.5 & 0.5 \\ \hline
%    \end{tabular}
%    \end{scriptsize}
%    \label{tb:single-SIFT-query}
%\end{center}   
%\end{table}

\section{Evaluation}
\label{sec:Perf}
In this section, we present the performance measurements of SystemL from various aspects.
We demonstrate various built-in indexing strategy for geometry mapping, and profile the tradeoff
between index building time, query performance, memory usage and storage consumption.
We present the end-to-end performance and memory/disk usage for three real ML pipelins.
Then we show the profiling of SystemL's scalability with three real ML pipelines on varying cluster sizes.
Last, we measure the performance of the three use cases presented in \S\ref{sec:Back-cases} with 
fine-grained lineage and its according cost.

Unless otherwise specified, we run all experiments on Amazon EC2 using r3.8xlarge instances.
Each machine has 16 physical cores, 244 GB memory, and two 320GB SSD. 
SystemL is integrated with KeystoneML 0.3 which in turn runs over Spark 1.3.1, HDFS from the
CDH4 distribution of Hadoop. 

\subsection{Geometry Mapping Index}
\label{sec:Perf-Index}
With this experiment we seek to profile the characteristics of the indexing strategies for geometry lineage discussed in \S\ref{sec:GeometryIndex}.
We use a workload with 5,000 pairs of input and output matrix, with each pair producing 60,000 geometry mappings that are uniformly distributed
in 2D space. The total number of mappings has 300 million geometries. The experiment was executed on a 16 machine cluster.

Figure~\ref{fig:sift-time} presents the end-to-end performance of this workload with a detailed runtime decomposition. For each data point, 
we run it three times and present the average. 

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/SIFTIndex-Time}
\caption {Runtime profile of the indexing strategies for geometry mapping over 300 million geometry mappings.
    \label{fig:sift-time}
}
\end{center}
\end{figure}

Without lineage tracking, the workload takes 61.3~seconds to finish without lineage tracking, shown as the baseline.
Tracking metadata mapping (geometry mapping) incurs a 89.8~seconds overhead in average. 
Building the index for geometry mapping takes 33.9, 69.9, and 349.0~seconds for RTree, KMeans-1, and KMeans-5.
The Direct strategy crashes due to insufficient memory.

Figure~\ref{fig:sift-query} presents the query performance with these indexing strategies.
For each strategy, we run queries with {1, 100, 1000, 10000} batch size.
NoIndex strategy perform worst among the four strategies, the query latency quickly grow over 10,000~ms with batch size of 100.
The other three strategies show a similar scalability, while KMeans-1 runs 17.9\% and 25.1\% faster than RTree with batch size of 1000 and 10000, respectively.
KMeans-5 runs  31.1\% and 35.2\% faster with batch size of 1000 and 10000, respectively.


\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/SIFTQuery-Time}
\caption {Query performance profile of the geometry mapping with four indexing strategies
    \label{fig:sift-query}
}
\end{center}
\end{figure}

The storage consumption of the four indexing strategies is 30.8GB, 55.4GB, 34.3GB, and 34.3GB 
for NoIndex, RTree, KMeans-1, KMeans-5, respectively. 
For RTree strategy, the index takes as much as 83\% of the mapping size. 
While for RTree, it is 11.4\%.
These four indexing strategies use 71.4\%, 85.7\%, 85.7\%, and 114.3\% more memory comparing to the
baseline case that does not track lineage.

To summarize, NoIndex has the lowest query performance and it is not practical for real use.
KMeans-5 has the best query performance the longest index building time.
RTree has a balanced query performance and building time with the highest storage consumption.
Since storage is not the major bottleneck given the machine capability, we use the RTree strategy
as default, leaving the RTree strategies as users' options.



\subsection{End-to-end Pipeline Performance}
We use three ML pipelines to measure the end-to-end performance and its penalty with lineage capturing
along with resource consumption of memory and storage on a 16 machine cluster.
For each pipeline, we collect both the metadata mapping, the input dataset of the first transformer,  and the output dataset of all transformers.
Table~\ref{tb:apps-overhead} presents the measure numbers, for each data point in the table, it is an
average of three executions. 

We see the performance penalty ranges from 25.6\% to 62.2\%, this performance penalty is comparable to that
of previous works of GMRW (16$\sim$76\%), Newt (20$\sim$50\%), and SubZero(20\%$\sim$150\%).
The memory usage overhead comparing to the baseline case is 200\% for MNIST pipeline, however it is using only
193~GB out of 3.9~TB memory space. 
For more memory-intensive pipelines of SIFTFisher (1.3~TB) and SourceExtractor (1.6~TB), the additional usage is 38.5\% and 43.7\%, respectively.

\begin{table}[ht]
\begin{center}
    \caption{Overhead Profile of SystemL on the SIFTFisher, SourceExtractor, and MNIST Pipelines}
    \begin{scriptsize}
    \begin{tabular}{ | p{1.8cm} | p{1.8cm} | p{1.8cm} | p{1.5cm} | }
    \hline
    Pipeline & SIFTFisher & SourceExtractor & MNIST  \\ \hline \hline
    Baseline & 780.8s & 279.7s & 56.4s \\ \hline
    Perf Penalty & 38.4\% & 25.6\% & 62.2\%   \\ \hline
    Memory & 38.5\% & 43.7\% & 200\%\\ \hline
    Meta Space & 109.9~GB & 204.4~MB & 78.9~MB \\ \hline
    Data Space & 615.7~GB & 506.9~GB & 11.8~GB\\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:apps-overhead}
\end{center}   
\end{table}

\subsection{Scalability}
We use the same three ML pipelines and execute them on clusters with {8, 16, 32, 64} of machines.
We measure the performance of the baseline (without lineage), lineage with only metadata mapping, 
and lineage with both metadata mapping and datasets. 
Then we decompose the running time for NoLineage, Meta, and Data, and show the relative scalability
 to the performance of 8 machines, shown as Figure~\ref{fig:scalability}.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/Scalability}
\caption {Relative Speedup of the VOCSIFTFisher, Source Extractor, and MNIST Pipelines with SystemL over Eight Nodes.
    \label{fig:scalability}
}
\end{center}
\end{figure}

One pattern we can conclude from the scalability measurements is that SystemL scales no worse than the ML pipelines.
This is due to the fact that SystemL captures and stores the lineage information via the RDD abstraction, which perform
in a scalable manner. 
The metadata mapping capturing is more likely to be impacted by the ML pipeline comparing to the case where both metadata
mapping and datasets are captured, since it is a relatively less data-intensive operations.
Combining pipeline stats shown in Table~\ref{tb:apps-overhead}, SystemL scales  well when collecting larger datasets.
In the SIFTFisher case, capturing both metadata mapping and datasets shows scalability that is higher than ideal.
This is because the SIFTFisher performance is memory-bounded on 8 machines, once the memory bound is relieved, the actual performance
exceeds the expectation.

\subsection{Query Performance}
To understand the query performance and scalability of each lineage type, we run queries with a varying size of keys
and present the performance in Figure~\ref{fig:typequery}. For each lineage type, we use a distributed collection (RDD) of 5,000 pairs of 
input and output data structures (vector, matrix, image) and distribute them into 200 partitions over 16 machines.
Table~\ref{tb:typequery-stats} shows the metadata settings of the input and output data structure in each query

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/TypeQuery-Time}
\caption {Query time for the seven lineage types (in log scale).
    \label{fig:typequery}
}
\end{center}
\end{figure}

\begin{table}[ht]
\begin{center}
    \caption{Data Structure Settings}
    \begin{scriptsize}
    \begin{tabular}{ | p{1.5cm} | p{3cm} | p{3cm} |}
    \hline
    Type & Input Structure & Output Structure \\ \hline \hline
    All & Vector(40960) & Vector(40960) \\ \hline
    Identity & Matrix(80, 512) & Matrix(80, 512) \\ \hline
    LinCom & Vector(40960) & Vector(20) \\ \hline
    Expansion & Matrix(80, 512) & Vector(40960) \\ \hline
    Collapse & Image(375, 500, 3) & Matrix(375, 500) \\ \hline
    Join & Matrix(500, 300) & Matrix(300, 500) \\ \hline
    Geometry& \shortstack[l]{Matrix(300, 500), \\60K geometries} &  \shortstack[l]{Matrix(128, 60000), \\60K geometries} \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:typequery-stats}
\end{center}   
\end{table}

Given the {\bf All Mapping}, {\bf Identity Mapping}, {\bf LinCom Mapping}, {\bf Expansion Mapping}, {\bf Collapse Mapping}, {\bf Join Mapping}
only involves trivial computation, queries with key size from 1 to 10K show latency under 200~ms which is generally the Spark operating latency. 
These queries show performance degradation with 100K keys (generally around 1~second). 
From 100K key size, the query turnaround time increase proportionally with the key size as shown with the 1M key performance.
{\bf Geometry Mapping} shows a consistent turnaround time under 200~ms from 1 to 1K key size. 
Beyond that, query with 10K keys takes $2.41\pm0.6$~seconds and query with 1M keys take $132.9\pm0.4$~seconds

This query scalability profile shows low latency ($\sim$200~ms) for an individual element investigation of single transformer. 
In the chained lineage cases, an individual element query can return a large number of coordinates, e.g., on the order of 1M. 
The subsequent query with this 1M coordinates can then incur latency on the order of seconds, which quick accumulates the
overall turnaround time higher than the 10~second boundary mentioned in ~\S\ref{sec:Req-Perf}. 
This is a problem for end-to-end pipeline lineage investigation, e.g., verifying intermediate data removal safety property~\ref{sec:formal-skipping}.
One such case on the MNIST pipeline with 16 chained transformers takes $1\pm0.1$~seconds for backward query and $1.0\pm 0.05$ for forward query.
While in the case of SIFTFisher pipeline,  it can take an estimated $O(10^3)$~seconds to finish as the backward query on the SIFTExtractor
transformer has a key size of 7.7M from a matrix with 128 rows and 60K columns to an image with 333 rows, 500 columns and 3 channels. 
For the moment, we put this large scale query optimization as future work.

\subsection{Use Case}
In the {\bf code validation} case, users instrument the SIFTFisher pipeline by only recording the metadata mapping of the SIFTExtractor transformer.
Upon validation, users can query extracted feature, and locate the positions of the feature in the input image. With some external visualization tool,
users can visualize corresponding pixels in the original image. Such an instrumentation introduces a performance penalty of 15.8\% on a 16 machine 
cluster.
Query a single feature takes $0.14\pm0.0062$~seconds while a batch query of 1,000 features takes $0.33\pm0.001$~seconds.

In the {\bf results inspection} use case, users of the SourceExtractor pipeline collects the metadata mapping in all transformers along with the output 
datasets of the last step. By joining the metadata mapping with output datasets, which contains the luminosity information, users can first filter out the
objects with a threshold brightness. Then query the metadata mapping to find the positions of corresponding pixels in the input images.
This instrumentation introduces a performance penalty of 40.8\% on a 8 machine cluster. 
Filtering luminous flux $>300,000$~lumen over 1 million objects returns 10,283 objects
with its corresponding pixel positions in $1.84\pm0.14$~seconds.

In the {\bf data cleaning} use case, the user of the SIFTFisher pipeline want to remove the training images that has both the human and table as
he sees such a combination results in lower detection accuracy for the human category .
So he instruments pipeline collecting all metadata mappings and the output datasets for NormalizeRows transformer. 
Upon removal, he first joins the output dataset of NormalizeRows (a RDD of vectors) transformer with the labels.
Then he filters out the corresponding vectors with both cats and dogs in it ( eight vectors corresponding to eight images in this case). 
From here, he can try retrain the model with the cleaned dataset. 
This particular instrumentation introduces a performance penalty of 46.8\%, and the removal takes $25.7\pm0.27$ seconds.
Comparing to the case of rerunning the data preparation phase of the pipeline, using lineage information speeds up the turnaround
time by 16.4x (from $421.3\pm3.8$~seconds to $25.7\pm0.3$~seconds). 
Further, to confirm the safety of the removal, the users can query the lineage backward to find corresponding 
images and conduct a verification.

\section{Related Work}
\label{sec:Related}
Researchers have extensively studied data lineage (in some cases referred as data provenance) for various purposes.
For scientific workflow systems such as  Chimera~\cite{foster02}, Taverna~\cite{oinn02}, and ESSW~\cite{frew01}, 
collects coarse-grained lineage of file metadata and according computation. 
Similar work has been well summarized by survey~\cite{simmhan05, freire08, bose05}
Recent work~\cite{altintas10} investigates how to integrate the lineage of workflow executions
in a collaborative environment. Coarse-grained lineage is useful in these systems to trace the data and code dependencies.
Since the lineage collected is coarse-grained, it is not sufficient for cell level diagnosis of ML pipelines.

Fine-grained lineage has been investigated in the context of data visualization~\cite{stonebraker93, woodruff97},  
data warehouse~\cite{cui00, cui03}, RDBMS~\cite{widom04}, and user-curated database~\cite{buneman06}.
Recent works of GMRW~\cite{ikeda11} and Newt~\cite{logothetis13} collects fine-grained lineage for the mapreduce
systems. SubZero~\cite{wu13} proposes a prototype lineage system for array-based database system SciDB~\cite{brown10}.
All these systems employ a bottom-up approach to design lineage capturing interface as the underlying systems all have well regulated low level operators.
For example, the data warehouse has the aggregate, select, project, join operators. 
RDMBS has its SQL operators. Mapreduce systems formulates the computation as mappers and reducers.
The curated database defines four operators of insert, delete, copy and paste.
One of SubZero's contributions is the region lineage that collects fine-grained lineage for UDFs besides the built-in operators in SciDB. 
In contrast, an ML pipeline can be viewed as a chain of user-defined data transformations, and the users often have the knowledge 
of lineage types and data of interest since they are the designers of the pipeline. 
Though lineage capturing interface designed with the bottom-up approach is general to support applications running on top these systems,
it exposes very limited flexibility of lineage type declaration and instrumentation to users.

Researchers also have tried to collect fine-grained lineage with other approaches.
Weak inversion and verification methods~\cite{woodruff97} are proposed to support approximate lineage collection.
Dynamic program analysis is employed to capture fine-grained lineage inside non-relational operators~\cite{zhang07} with 
a 7.5$\sim$39.8x slowdown comparing the original program. 
These approaches do not fit the ML pipeline lineage scenario since they either loose the lineage accuracy or introduce
excessive performance penalty.
The Arnold system~\cite{devecsery14} captures fine-grained lineage at operating system process level by recoding
all non-deterministic data such as the order, return values, modified memory addresses, and many others. 
This approach can collect fine-grain lineage for any process running on top of the operating system, however
it looses track of the upper level data structures (e.g., vector, matrix, images), which are fundamental to ML pipelines.
One step beyond fine-grained lineage, researches~\cite{meliou10, meliou11} explores the causality and responsibility
of inputs to the results. Though the causality and responsibility is relatively easier for data transformations in ML pipelines.
It can be applied in the training process analysis to infer the contribution of individual training sample to the model and predictions.


\section{Conclusion and Future Work}
\label{sec:Conclusion}
To enable accurate, flexible, low-overhead lineage collection and responsive lineage query, we present the SystemL lineage system.
SystemL exposes a concise yet powerful lineage capturing interface for users by formulating the ML pipeline lineage as a sequence
of multi-dimensional space transformations. The proposed interface has a coverage of 87\% of the transformers and estimators
in the current KeystoneML code base. Combining the metadata of data structures, the high-order function approach and spatial indexing
strategy, SystemL is able to answer typical query within $\sim$100~ms and $\sim$1~second. Practically, SystemL introduces a
performance penalty of 16\%-54\% depending on the pipeline and use case.

As future work, we are investigating the diagnosability of the training process in the estimator. We seek to provide the users with
the fine-grained lineage information inside an estimator to answer queries of supporting training samples for
a prediction and the impact of certain feature removal or training data removal.

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP,  The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple, Inc., Blue Goji, Bosch, C3Energy, Cisco, Cray, Cloudera, EMC, Ericsson, Facebook, Guavus, Huawei, Intel, Microsoft, NetApp, Pivotal, Samsung, Splunk, Virdata, VMware, and Yahoo!. 

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{Lineage} % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns



\balancecolumns

% That's all folks!
\end{document}
