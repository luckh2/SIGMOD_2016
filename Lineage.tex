% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

%\documentclass{acm_proc_article-sp}
\documentclass{sig-alternate}
\usepackage{color}
\usepackage{multirow}
\usepackage{listings}
\usepackage{url}
\usepackage{listings}
\lstset{language=sql, escapeinside={\$}, basicstyle=\footnotesize\ttfamily, mathescape, escapeinside={\%*}{*)}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]

\newif\ifdraft
\drafttrue
%\draftfalse                                                                              
\ifdraft
\newcommand{\zhaonote}[1]{{\textcolor{cyan}    { ***Zhao:      #1 }}}
\newcommand{\evannote}[1]{{\textcolor{red}    { ***Evan:      #1 }}}
\newcommand{\note}[1]{ {\textcolor{red}    {\bf #1 }}}
\else
\newcommand{\zhaonote}[1]{}
\newcommand{\evannote}[1]{}
\newcommand{\note}[1]{}
\fi

\newenvironment{shortlist}{
        \vspace*{-0.5em}
  \begin{itemize}
  \setlength{\itemsep}{-0.1em}
}{
  \end{itemize}
        \vspace*{-0.5em}
}

\begin{document}

\title{SystemL: Diagnosing Machine Learning Pipelines with Fine-grained Lineage}

\numberofauthors{1} 
\author{
% 1st. author
\alignauthor Zhao~Zhang, Evan~R.~Sparks, Michael~J.~Franklin \\\
       \affaddr{AMPLab, University of California, Berkeley}\\
       \email{\{zhaozhang, sparks, franklin\}@cs.berkeley.edu} 
}

\maketitle

\begin{abstract}
We present the SystemL lineage system to enable the diagnosis of distributed machine learning (ML) pipelines by leveraging the fine-grained
data lineage.  
SystemL exposes a concise yet powerful API, derived from \emph{primitive lineage types}, and collects fine-grained data lineage for each 
data transformation by recording the input datasets, the output datasets and the mapping between them, along
with all information needed to reproduce the computation.
SystemL efficiently enables data anomaly removal and computation replay, code debugging, and result analysis.
For a single transformation, SystemL's high-order function mapping strategy can reduce the storage consumption by up to 
45x comparing to the element-wise mapping recording.
The spatial index allows SystemL to speed up query performance by a further 2x-31x. 
The performance penalty introduced by SystemL is 16\%-54\% depending on the pipeline and use case.
\end{abstract}

% A category with the (minimum) three required fields
\category{Scalable Data Analysis}{System for Data Management}[Big Data]
%\keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
Machine learning frameworks are increasingly popular as practitioners and researchers can quickly
build applications (referred to as ML pipelines in the rest of this paper or pipelines in the rest of this paper) with a high-level 
programming language to pipeline data preparation, feature extraction, model training, and prediction. 
Among these systems, Scikit-learn~\cite{pedregosa2011scikit}
is a single-computer-based framework while TensorFlow~\cite{tensorflow15}, Apache Mahout~\cite{owen2011mahout}, MLlib~\cite{meng2015mllib}, 
SystemML~\cite{ghoting11systemml}, and KeystoneML~\cite{sparks15} focus on a distributed environment.
In practice, to obtain a working model, users often need to try a number of options (e.g. featurization, training parameters, and datasets). 
When experimenting with these options, users frequently analyze results together with the corresponding input data,  
locate the code segment for unexpected outcome, and rerun the computation with the removal of suspicious data anomalies.
Commonly, they use a case-by-case solution to record necessary information and store such information in persistent storage for examination.
These solutions are often use case specific, performance degradation prone, and they can complicate code
management due to the demand for multiple versions of a pipeline.
Given that this is a repeated pattern taken on by users of the system, it is natural to add support 
that is flexible for users in terms of lineage specification, instrumentation and query.

In this paper, we describe the design of a diagnostic system for distributed ML frameworks by leveraging the fine-grained lineage information
(data lineage at a structure cell level, e.g. elements in a matrix). 
We assume an ML framework with a programming model that abstracts a pipeline as a chain of data transformations, such as KeystoneML
which builds on top of Apache Spark, and use two building blocks: transformers and estimators.
A transformer applies a deterministic unary function to data items and produce new data items.
An estimator takes a collection of data items, and feed them to a training procedure,  and produces a transformer.
We choose this model because users can declare and instrument lineage collection in the transformer.
This diagnostic system enables users to specify the data transformation lineage with a concise interface, customize the instrumentation, 
collect fine-grained lineage with a minimal performance penalty, and serve the input/output dependency query with a low latency. 

%answer sophisticated lineage query responsively( with query latency lower than 10~seconds~\cite{nielsen2009}).
Lineage tracking has been investigated in various areas.
Existing lineage tracking system such as Chimera~\cite{foster02} and and Taverna~\cite{oinn02} can collect coarse-grained lineage of scientific workflow systems.
The weak inversion method~\cite{woodruff97} for RDBMS was proposed for queries that can tolerate inaccurate lineage information.
Trio~\cite{widom04} tracks lineage for the RDBMS with cell mappings materialization at the same time of view queries.
Recent work in fine-grained lineage capturing such as GMRW~\cite{ikeda11} and Newt~\cite{logothetis13} presents
solutions for MapReduce systems, and capture lineage on low level operators such as mappers and reducers.
SubZero~\cite{wu13}, which is built on SciDB~\cite{brown10} has a similar lineage capturing interface at operators and UDFs.

Unfortunately, none of the existing lineage tracking systems is suitable for the distributed ML frameworks for two reasons.
First, the lineage information collected by scientific workflow systems (e.g., Chimera) and the weak inversion method is not sufficient for accurate ML pipeline diagnosis.
Second, the lineage capturing interface of Trio, GMRW, Newt, and SubZero is designed with a bottom-up approach
that operates on the underlying system operators (e.g., mappers and reducers in MapReduce systems, UDFs in SciDB). 
This approach is general for all workloads running on the system, but it only gives users very limited flexibility for customized
instrumentation (specification of lineage type and the data to be collected) in the case when users know the exact data to collect.

In this work, we use a top-down approach to design the lineage capturing interface that returns declaration and instrumentation
flexibility to users. By formulating the cell level mapping of an ML pipeline as a sequence of multi-dimensional space transformations
(Figure~\ref{fig:conceptual} shows an conceptual illustration), we are able to identify seven primitive lineage types.
Higher-dimensional space can collapse/expand to lower-dimensional spaces along one dimension at a time, 
resulting in {\bf collapse} mapping(\textcircled{1}) and {\bf expansion} mapping(\textcircled{6}, \textcircled{7}). 
Mappings between spaces with the same number of dimensions can be {\bf identity} mapping(\textcircled{5}), {\bf all} mapping(\textcircled{9}), 
{\bf geometry} mapping(\textcircled{2}, \textcircled{4}), {\bf lincom} (short for linear combination) mapping(\textcircled{3}, \textcircled{8}),
and {\bf join} mapping. 
This interface covers 87\% (39 out of 45) of the data transformations in the current code base of KeystoneML. 
For transformations that are not covered, the lineage capturing interface allows users to specify a customized mapping function.
A formal definition of the mapping types is in \S\ref{sec:Design-Mapping}.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/Conceptual}
    \caption {A synthetic image classification pipeline with elements mapping highlighted in color scheme. 
    \textcircled{1}: Conversion of multi-channel image to gray scale.
    \textcircled{2}: Local feature extraction.
    \textcircled{3}: Vector dimension reduction.
    \textcircled{4}: Vector sampling.
    \textcircled{5}: Conversion from float to double.
    \textcircled{6}: Vector combining.
    \textcircled{7}: Matrix vectorizing.
    \textcircled{8}: Lineage model prediction.
    \textcircled{9}: Selection of the index with maximum value.
    \label{fig:conceptual}
}
\end{center}
\end{figure}


%Diagnosing such an ML pipeline with datasets in a distributed environment can be problematic due to
%the lack of system support of memory states capturing and user interaction.
%To motivate, we present three typical use cases for ML pipeline diagnostics:
%\begin{shortlist}
%\item{\bf Code Validation}: In an image classification pipeline, a user wants to verify the correctness of 
%the positional information of features. 
%\item{\bf Results Inspection}: In an astronomical object extraction pipeline, an astronomer wants to find objects whose
%brightness are above a threshold then locate them in the input images.
%\item{\bf Data Cleaning}: In an image classification pipeline, a user observes a particular category has
%a low prediction accuracy, so he wants to remove the training samples of this category and retrain
%the model.
%\end{shortlist}

%The astronomer of the {\bf Results Inspection}
%case can first filter the results and find these objects that are above the brightness threshold, then query the lineage
%backward to find out the input pixels. With further mathematical analysis or visualization, the astronomer can tell
%if it is an object or an false detection introduced by cosmic ray. 
%The user of the {\bf Data Cleaning} case can trace the propagation of the images through the pipeline right before
%the featurized dataset is fed into training. He then can remove the corresponding data items that are derived from
%the low-accuracy category and replay the training process withe the clean data without repeating the whole pipeline.



%The performance penalty introduced by lineage capturing comes mostly from persisting in-memory data to storage.
%In the context of data transformation, there are three potential datasets: the input, the output, and mapping between them.
%Recording only the metadata (coordinates) mapping rather than the actual data value places a lower bound of the amount
%of data the lineage system needs to capture. The actual dataset can be derived by rerunning the pipeline.
%Among the mapping types, geometry mapping takes more storage space than others due to the arbitrary between two collections
%of metadata. We introduce an high-order function approach to describe each geometry and record the mapping between
%each pair of geometries to reduce the storage consumption. We also investigate various indexing strategies for better
%query performance for geometry mapping.

We present SystemL, a lineage collecting and serving system that implements the above general solutions.
SystemL is integrated with the KeystoneML~\cite{sparks15} distributed machine learning framework.
SystemL interacts with KeystoneML at the transformer level, where the users can declare the lineage type and instrumentation.
SystemL exploits the  data structure metadata (e.g., the size of each dimension of a space) for mapping recording to reduce storage overhead.
It also uses a high order function approach and spatial index for geometry mapping which does not have a regular mapping pattern.
SystemL collects five types of data for each transformer: the input dataset, the output dataset, the mapping between them
along with the computation and the optional model.
SystemL stores the lineage information on HDFS~\cite{shvachko10} via the resilient distributed dataset (RDD) abstraction
of Apache Spark~\cite{zaharia12}, the underlying computing engine of KeystoneML.
Unlike Spark lineage, which tracks partition level data dependency and associated computation for internal system resilience,
SystemL tracks lineage with a finer granularity at the cell level of data structures.
In addition, SystemL exposes an interface for users to specify lineage types and query.
When queried, SystemL can load the lineage information from storage and reconstruct the lineage chain.
Users have the flexibility to reconstruct a single transformer lineage, a partial chain, or the whole chain.


Combining the high-order function approach and spatial indexing strategies, 
the geometry lineage query is improved by 2x-31x.
Storage consumption is reduced by up to 45x compared to the element-wise mapping recording. 
Our measurements with the three representative ML pipelines show that SystemL lineage 
capturing scales no worse than the ML pipelines.
The performance penalty for three typical use cases (discussed in \S\ref{sec:Back-cases}) ranges from 16\% to 54\%.
The query for these use cases takes from 0.1~s to 1.8~s. 

In summary, we make the following contributions:
\begin{shortlist}
\item{} We present a concise and powerful lineage system interface by formulating the cell-wise mapping in ML pipelines as multi-dimensional space transformation.
\item{} We present a high-order function to describe the geometry to reduce storage overhead.
\item{} We present and evaluate various spatial indexing strategies to improve the query performance.
\item{} We present three real use cases of using fine-grained data lineage for code validation, results inspection, and data cleaning, respectively.
\end{shortlist}


\section{Use Cases and KeystoneML}
\label{sec:Background}
In this section, we present three motivating use cases of SystemL and briefly review KeystoneML.

\subsection{Use Cases}
\label{sec:Back-cases}
We select three use cases from real ML pipelines, these three cases are representative for code validation, results inspection, and data cleaning, respectively.

\subsubsection{Code Validation}
In this use case we use lineage as a tool in the debugging process as pipelines change and are augmented. 
Typical machine learning workflows are constructed via an iterative proces of refinement with a machine learning developer or data scientist in the loop.
These users are constantly engineering new features, adding new datasources, and trying out new machine learning methods at all stages of the pipeline.
Lineage offers a natural way for these users to hone in on the areas where two similar pipelines diverge in terms of their intermediate data, and presents a new avenue for investigation of model performance by allowing users to pinpoint the point at which their data diverged from a known good pipeline and to inspect \emph{what} changed during the data processing procedure. 


\begin{figure}[ht]
\begin{center}
    \includegraphics[width=85mm]{pictures/VOCSIFTFisher}
    \caption {The SIFTFisher pipeline, boxes are dataset, rounded-corner boxes are transformers, dashed rounded-corner boxes are estimators, shaded boxes are calling out to C library for computation.
    \label{fig:vocsiftfisher}
}
\end{center}
\end{figure}

As a concrete example, consider the SIFTFisher pipeline, shown as Figure~\ref{fig:vocsiftfisher}. 
It has been shown that augmenting traditional SIFT~\cite{lowe99} (Scale-invariant feature transform) descriptors with additional location information about the location of the descriptor in the input image can improve classification performance.
However, when we first developed such a pipeline, our results conflicted with published work indicating that these features provided a statistically significant improvement in classification accuracy.
By employing lineage based debugging, we could isolate exactly where in the pipeline our calculations diverged from the original features, and diagnose impacts seen downstream like a under-fit GMM (Gaussian Mixture Model), and trace the ultimate lack of classification improvement to a bug in the underlying C library which we called into.
By visualizing the features we could see that the new features added to our model did not correspond to positions in the image, but rather contained values outside of a suitable range for these features, which turned out to be randomly allocated memory. The SIFT feature visualization involves backward query on the lineage of the SIFTExtractor transformer and highlights of the corresponding input pixels.


\subsubsection{Results Inspection}
Users of ML pipelines often interpret the results, identify some potential data anomalies, then retrospect the supporting input data for better understanding.
Fine-grained data lineage of ML pipelines have sufficient information for such investigation.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/SourceExtractor}
    \caption {The SourceExtractor pipeline, boxes are dataset, rounded-corner boxes are transformers, dashed rounded-corner boxes are estimators, shaded boxes are calling out to C library for computation.
    \label{fig:sourceextractor}
}
\end{center}
\end{figure}

One such use case is results validation in the SourceExtractor pipeline, shown in Figure~\ref{fig:sourceextractor},
that processes images from the telescopes and produces astronomical object catalogs. 
The astronomical object catalog includes the position, shape, and other statistical properties of the object.
In this case, astronomers find the bright objects over a threshold and validate that these objects are indeed astronomical objects rather
than noise caused by cosmic rays.
With the lineage information, astronomers can first filter the results to find the objects whose brightness is above a threshold.
Then using backward query, they can find corresponding pixels for these bright objects. 
Either through mathematical analysis or human intervention, they are able to validate the correctness of the bright objects.

\subsubsection{Data Cleaning}
\label{sec:Back-Case-Cleaning}
Another use of the lineage information is for users to apply a subset of the prepared dataset to fit the model. 
A user of the SIFTFisher pipeline, shown in Figure~\ref{fig:vocsiftfisher}, observes a low prediction accuracy for a category of images, especially 
when people and dogs are in the same image. A natural investigation is then to remove the images with both people and dogs,
and test if that improves prediction accuracy.

Without lineage information, the user has to remove the according images from the training set and reruns data
preparation part (from PixelScaler to RowNormalizer), then feed the dataset to linear solver to train the model.
With lineage information, the user can intercept the pipeline by loading the output of RowNormalizer directly into KeystoneML,
filter out the data items that are derived from the images that have people and dogs together , and apply the filtered dataset directly to the linear solver.
In this case, the rerunning process can bypass the expensive data preparation procedure resulting in shorter turn-around time.

\subsection{KeystoneML}
KeystoneML is an application framework designed for the implementation of robust large-scale machine learning pipelines. Built on the principles of declarative programming and modular design, KeystoneML provides a light-weight and elegant API that allows users to describe these pipelines as the composition of two types of operator--transformers, which perform deterministic data transformation, and estimators, which ``learn'' transformers based on training data. KeystoneML pipelines are fit and executed in parallel using Apache Spark. These pipelines are compiled into an application DAG and optimized before execution. Current optimizations include online decisions about materialization of intermediate state, as well as standard optimizations such as common subexpression elimination. KeystoneML includes a library of standard feature extractors in domains including computer vision, audio, and text processing, as well as standard statistical procedures and Estimators for several types of machine learning model.

\section{Formal Discussion}
\label{sec:Mapping}
In this section, we formally define the notion of ML pipeline, dataset, mapping and its associated operations of query in the context of 
relational database.

\subsection{Pipeline and Datasets}
\label{sec:Map-Pipe-Data}
We define a dataset $I$ as a collection of structured data. In the scope of this work, the supported structures are: 
string, vector, matrix, and image. A vector or a string is a one dimensional data structure and a matrix is two dimensional. 
For simplicity, we view string as a vector.
An image is a three dimensional data structure that is defined by the height, the width and the number
of channels. 

An element $e \in I$ has two properties: value $e.value$ and coordinate $e.coor$ (referred as metadata or meta
in the following discussion). 
Combining the additional dimension in the collection (RDD or sequence) with the data structure, 
the metadata of an element is a pair of integers, a triple of integers, or a quadruple of integers
for vector, matrix, and image, respectively.

A pipeline $P$ is defined as a sequence of transformers $(T_1, T_2, ..., T_n)$. 
Each transformer $T_i$ takes a dataset $I_i$ as its input, and produces $I_{i+1}$ as the output: 
$I_{i+1} = T_i(I_i)$. 
Especially, we denote the output of the whole pipeline as $O = I_{n+1}$

To simplify the discussion, we use the vector data structure as example in the following discussion.
Other data structures can be defined in a similar way. 
For queries, we present the definition of forward query and backward query can be inducted by reversing
the table order in the join operation.

\subsection{Dataset and Mapping}
Conceptually, the input dataset and output dataset can be defined
as tables with each element's position in the data structure as key.
In reality, such tables is in the form of a distributed collection such as RDD.
\newline\newline
\noindent\begin{minipage}{\textwidth}
\noindent\begin{minipage}{.2\textwidth}
\begin{verbatim}
CREATE TABLE input
(
  Meta (INT, INT),
  Value (DOUBLE)
);
\end{verbatim}
\end{minipage} 
\qquad{\color{black}\vrule}\qquad
\begin{minipage}{.2\textwidth}
\begin{verbatim}
CREATE TABLE output
(
  Meta (INT, INT),
  Value (DOUBLE)
)
\end{verbatim}
\end{minipage}
\end{minipage}
\vspace{2ex}

The mapping between the input and output dataset is a left outer join of the the two tables with a mapping function.
The Fun function returns the corresponding output elements given an input element. 
It is a left outer join since an input element may not be corresponding to any output element.
\begin{lstlisting}
SELECT *
From input
LEFT OUTER JOIN output 
    ON Fun(in.meta) $\ni$ out.meta
\end{lstlisting}

In practice, we separate the metadata from the actual element values and only record metadata mapping.
\begin{lstlisting}
INSERT INTO mapping (in_meta, out_meta)
SELECT in.meta, out.meta
From input
LEFT OUTER JOIN output 
    ON Fun(in.meta) $\ni$ out.meta
\end{lstlisting}
The retrieval of actual element values can be obtained by joining the mapping table and the output dataset table.
\begin{lstlisting}
SELECT mapping.meta, out.value
From mapping
LEFT OUTER JOIN output 
    ON mapping.out_meta = output.meta
\end{lstlisting}

The input and output mapping of a pipeline is in turn a sequence of mappings of individual transformer.
Consider a pipeline $P=(T_1, T_2, ..., T_n)$, we denote $mapping_i$ as the mapping for transformer $T_i$.
The mapping of the pipeline is a sequence of left outer join operations of $\{mapping_1, ..., mapping_n\}$ on 
$mapping_i.out\_meta = mapping_{i+1}.in\_meta$.

\subsection{Mapping Query}
A forward query with a key  ({\it qForward(key)}) over the mapping of a transformer or a pipeline can be expressed as:
\begin{lstlisting}
SELECT out_meta
FROM mapping
WHERE key = in_meta
\end{lstlisting}

A forward query with a list of $keys={k_1, k_2, ..., k_n}$ ({\it qForward(keys)}) can be expressed as:
\begin{lstlisting}
SELECT out_meta
FROM mapping
WHERE $k_1$ = in_meta OR ... OR $k_n$=in_meta
\end{lstlisting}

\subsection{Data Preparation Skipping}
In use cases such as the data cleaning presented in \S\ref{sec:Back-Case-Cleaning}, users want
to remove certain suspicious input data items that might relate to bad prediction results and retrain the model then
test its performance. 
Repeating the data preparation steps (e.g., from PixelScaler to RowNormalizer in SIFTFisher pipeline) incurs
long running time.
One natural thought in such case is ``can we skip the computation applied on the unchanged data items by removing
the corresponding prepared data items directly?''

Formally, given a data transformation {\it f()} applied on a collection $C$ of vectors \{$v_1, v_2, ..., v_n$\}.
we say it is safe to remove {\it f($v_i$)} when {\it $f(C/v_i)) = f(C)/f(v_i))$}.
The notion {\it $C/v_i$} can be interpreted as the collection derived by removing $v_i$ from $C$.
In other words, function {\it f()} is distributive on vectors \{$v_1, v_2, ..., v_n$\}. 
With single or few transformers, users are able to track the distributive property.
However when the pipeline is deep, such property is hard to track.

Using the fine-grained data lineage, we present a validation method for the distributive property.
We say the mapping of {\it f()} is self-reflected if $qBackward_f(qForward_f(v_i)) \subseteq v_i$, where vector $v_i$ can be viewed 
as a set of one-dimensional elements.


\begin{theorem}
\label{thm:distributive}
A data transformation f() is distributive if the mapping of f() is self-reflected.
\end{theorem}

\begin{proof}
By contradiction, if f() is not distributive on collection $C$ of vectors  \{$v_1, v_2, ..., v_n$\}, then $\exists e \in v_j $ s.t. $f(v_i)$ takes as input.
Then $e \in qBackward_f(qForward_f(v_i))$ and $e \in v_i$ which results in  $qBackward_f(qForward_f(v_i)) \nsubseteq v_i$, which violates condition.
\end{proof}

\begin{corollary}
A sequence of data transformations ($s = {f_1, f_2, ..., f_n}$) is distributive if the mapping between the very input and output datasets is self-reflected.
\end{corollary}

\subsection{Mapping Function}
\label{sec:Map-Func}
The mapping function {\bf Func} contains the core information of the input and output element dependency.
Recording the element-wise dependency can take up to $O(kN^2)$ space given k N-dimensional vectors in the case
of every output element depends on every input element. 
To eliminate this space burst, we identify seven primitive mapping functions.
From higher dimension space to lower dimension space, the mapping can be {\bf collapse} or {\bf expansion}.
Between the equal dimension spaces, the mapping can be {\bf identity}, {\bf all}, {\bf geometry}, {\bf lincom}, and {\bf join}.
Figure~\ref{fig:narrowmapping} shows the seven primitive mapping functions, with the mapping highlighted with color scheme.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/narrowmapping}
\caption {Seven primitive mapping functions.
    \label{fig:narrowmapping}
}
\end{center}
\end{figure}


{\bf Collapse} mapping describes the mapping of a space collapsing into a one dimension lower space along one dimension which needs
to be specified by the user. Examples include gray scaling a multiple channel image.
{\bf Expansion} mapping is where a space expands to a one dimension lower space, e.g. expanding a matrix to a vector with column major.
In {\bf Identity} mapping, each output element only depends on one element with the same coordinate in the input matrix.
In {\bf All} mapping, each output element depends on all elements in the input matrix.
{\bf Geometry} mapping describes the case where a group of output elements depend on a group of input elements.
{\bf LinCom} mapping describes the data dependencies in a linear model.
{\bf Join} mapping happens between equal dimension space transformation, where nested collection of data structure is reorganized by 
transposing two dimensions
For the mappings that are not covered by the primitive functions, we allow users to define their own mapping functions.

%Since transformers operate at the data structure (vector, matrix, image) level, we use the notation
%$\overline{I_i(e)}$ as the companion of $I_i$ with respect to element $e$ such that $\forall a \in \overline{I_i(e)}$,
%\[ a.value =
%  \begin{cases}
%    e.value       & \quad \text{if } a.coor = e.coor\\
%    NaN  & \quad \text{otherwise}. \\
%  \end{cases}
%\]

%Accordingly, given a list of distinct elements $(e_1, ..., e_m)$, the companion of $I_i$ with respect to $(e_1, ..., e_m)$ is
%described as: $\forall a \in \overline{I_i(e_1, ..., e_m)}$, 
%\[ a.value =
%  \begin{cases}
%    e.value       & \quad \text{if } \exists e \in (e_1,...,e_m), a.coor = e.coor\\
%    NaN  & \quad \text{otherwise}. \\
%  \end{cases}
%\]

%Before defining the lineage, we need to introduce the relationship between the two data structures $A$ and $B$ with the same type.
%We say $A \subseteq B \text{ or } B \supseteq A$, $\text{if }\forall a \in A, \exists b \in B, s.t.\text{ } a \neq NaN \land a.coor = b.coor \land a.value = b.value$.
%By saying $A = B$, we mean $A \subseteq B$ and $B \subseteq A$.
%
%We define the data lineage of a given element as the elements in the input dataset that is used to produce present element.
%The single transformer lineage is
%\begin{equation}
%S(e \in I_{i+1}) = \{e_1, ..., e_m | T_i(\overline{I_i(e_1, ..., e_m)}) \supseteq \overline{I_{i+1}(e)}\}.
%\label{equa:SingleLineage}
%\end{equation}
%
%Recursively, the lineage of a given element across multiple transformers can be defined as
%\begin{equation}
%L(e) = \cup_{e' \in S(e)} L(e').
%\end{equation}
%
%We define the property 
%\begin{equation}
%Replay(e \in I_{i+1}) = \overline{I_{i+1}(e)} \subseteq T_i(\overline{I_i(e_1, ..., e_m)})
%\end{equation}
%as the replay property, which means we can reproduce an output element $e$ by applying the corresponding transformer to its data lineage.
%The constraint of the replay property is that only the element $e$ is guaranteed to be the same as the original transformation,
%if $\overline{I_{i+1}(e)} \subset T_i(\overline{I_i(e_1, ..., e_m)})$. 
%In this case, the results of the replayed transformer may produce inconsistent results for elements other than $e$ in $I_{i+1}$. 
%Similarly, the replay property for multiple elements only guarantees the correctness of these elements themselves.
%
%\subsection{Data Lineage Query}
%The forward query of an element over data lineage of a pipeline $P$ returns $qForward(e) = \{e' | e \in L(e' \in O)\}$. 
%And the backward query of an element over data lineage returns $qBackward(e) = e' \in L(e)$.
%Specifically, the identifier of an element is its coordinates in the according data structure, so the query actually takes
%the coordinates of the input element as key, and query over the data lineage. The query that spans multiple transformers 
%use coordinates as intermediate data. Once the query reaches the final transformer, it can associate the actual values
%with the result element coordinates and return them.
%
%Queries of multiple elements can then be expressed as the union of the query for each element: 
%\begin{equation}
%\begin{split}
%qForward(e_1, ..., e_m) = \cup_{e \in (e_1, ..., e_m)} qForward(e) \\
%qBackward(e_1, ..., e_m) = \cup_{e \in (e_1, ..., e_m)} qBackward(e).
%\end{split}
%\end{equation}

\section{Design Goals}
\label{sec:Req}
In this section, we present the design goals of SystemL from the functional and performance perspective.

\subsection{Functional Requirements}
Two basic functions of SystemL is lineage capturing and serving.
SystemL requires a programming interface between the users of KeystoneML and the lineage capturing unit to 
collect lineage information and customize instrumentation (deciding what dataset to store). 
This interface should be concise with a few variations and options.
This interface should also be powerful in terms of coverage of transformers.
This interface should be flexible so that users can define their own lineage and instrumentation.

To serve lineage for query, SystemL requires a user interface to reconstruct and chain collected lineage.
Each transformer should provide the forward query (qForward()) and backward query (qBackward()) functionality.
SystemL needs to verify the chained lineage has identical ordering to the pipeline.
The single transformer lineage should be able to be replayed deterministically, meaning that with the same input dataset and transformer,
the lineage contains enough information with which users can reproduce the results.
In turn, a partial pipeline assembled by a sequence of transformers of the original pipeline should be able to be replayed deterministically.

\subsection{Performance Requirements}
\label{sec:Req-Perf}
In general, as lineage capturing incurs additional computation and I/O for mapping materialization and storage, a non-trivial performance
penalty based on the original workload is expected. 
Table~\ref{tb:overhead} circulates documented performance penalty introduced by lineage capturing in recent works. 

\begin{table}[ht]
\begin{center}
    \caption{Performance Penalty Introduced by Lineage Capturing in Recent Works}
    \begin{scriptsize}
    \begin{tabular}{ | p{2.25cm} | p{2.25cm} | p{2.25cm} |}
    \hline
    Project & Underlying System & Performance Penalty  \\ \hline \hline
    GMRW~\cite{ikeda11} & Hadoop~\cite{HADOOP} & 16-76\% \\ \hline
    Newt~\cite{logothetis13} & Hadoop~\cite{HADOOP} & 20-50\% \\ \hline
    SubZero~\cite{wu13} & SciDB~\cite{brown10} & 50\%-150\% \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:overhead}
\end{center}   
\end{table}

This performance penalty problem exacerbates for SystemL, since KeystoneML's distributed computing engine Apache Spark 
optimizes for in-memory computing and do not write intermediate data to disks unless it is a shuffle operation. 
Comparing to systems such as Hadoop, the same amount of I/O of SystemL will introduce a more significant penalty.
Despite this obstacle, we expect SystemL's performance penalty to be comparable to the existing systems.

Lineage information can be massive depending on the pipeline, since the input or the intermediate dataset of a pipeline can be large.
In some transformers, the mapping between the input elements and output elements does not conform a pattern, e.g., Geometry mapping.
We need advance technique to reduce the storage burst introduced by such mappings.

Using SystemL for ML pipeline diagnosing requires a low-latency interaction between users and the lineage serving unit.
As documented in the report~\cite{nielsen2009} in human computer interaction, 1~second latency is the boundary to keep users' thought flow while 10~seconds
is the boundary to loose users attention.
We expect SystemL's query latency within the 10~seconds boundary to facilitate users diagnosis on ML pipelines.

\section{Design}
\label{sec:Design}
In this section, we discuss SystemL's layered design of lineage capturing and instrumentation interface, 

\subsection{Overview}
Figure~\ref{fig:architecture} shows the overview of SystemL and its interactions with other components. 
Rounded-corner rectangles are components around and inside KeystoneML. 
The two-sided arrows indicate interactions between components.
Users compose machine learning pipelines with KeystoneML and submit the compiled DAG to Spark.
All computation and I/O are done through the Spark Resilient Distributed Dataset (RDD) abstraction.
With SystemL, users can declare and instrument lineage at the transformer level, 
so that the compiled DAG also contains lineage recording and other operations, e.g., save to disk.
Lineage in SystemL is stored to HDFS~\cite{shvachko10} via Spark's RDD abstraction.
Users can load the stored lineage in HDFS to Spark interactive command line interface.
By interacting using the query interface, users can query elements of interest, replay transformation, or analyze
the lineage for other purposes.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/architecture}
\caption {Overview of SystemL and its surrounding components.
    \label{fig:architecture}
}
\end{center}
\end{figure}

\subsection{Lineage Interface}
\label{sec:Design-Lineage}
SystemL exposes the lineage capturing interface by naming the lineage type specific class with its mapping types between input and output datasets.
All the type specific class inherits an abstract class of Lineage, which define common methods shared among the type specific classes:
\begin{shortlist}
\item{} qForward(keys: List[Coor]): List[Coor], given a list of element metadata (coordinates) returns the forward query results on the metadata mapping of the lineage
\item{} qBackward(keys: List[Coor]): List[Coor], given a list of element metadata (coordinates) returns the backward query results on the metadata mapping of the lineage
\item{} saveMapping(): saves the collection of metadata mappings of a lineage to disks
\item{} saveInput(): saves the input dataset to disks
\item{} saveOutput(): saves the output dataset to disks.
\end{shortlist}


Table~\ref{tb:lineage-interface} summarizes the lineage types and parameters passing by the users.
\begin{table}[ht]
\begin{center}
    \caption{Primitive Lineage Types}
    \begin{scriptsize}
    \begin{tabular}{ | p{8cm}|}
    \hline
    CollapseLineage(in, out, transformer, dimension) \\ \hline 
    ExpansionLineage(in, out, transformer, dimension) \\ \hline
    IdentityLineage(in, out, transformer) \\ \hline
    AllLineage(in, out, transformer) \\ \hline
    GeoLineage(in, out, transformer, mappingCollection) \\ \hline
    LinComLineage(in, out, transformer, model) \\ \hline
    JoinLineage(in, out, transformer, dimension) \\ \hline
    CustomLineage(in, out, transformer, fFunc, bFunc, model, rand) \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:lineage-interface}
\end{center}   
\end{table}

In general, SystemL lineage capturing interface can collect six types of data: 
the input dataset (in), the output dataset (out), the transformer (transformer), 
the collection of mapping (mappingCollection), the model (model), and the random 
parameter (rand).

By checking the input and output dataset, SystemL is able to figure out the metadata of each data structure then use
it to declare the mapping objects, which is described in the following section.

\subsection{Mapping}
\label{sec:Design-Mapping}
Mapping on the abstract level provides two interfaces: qForward() and qBackward(). Both interfaces
take a list of element metadata (coordinates) and return the query results accordingly.

For all mapping types presented in ~\S\ref{sec:Map-Func} except Geometry mapping, 
SystemL infers metadata of data structures automatically. 
For example in an All mapping between two matrices of 5 rows and 5 columns,
the mapping is recorded as AllMapping(twoDMeta(5, 5), twoDMeta(5,5)). 
So upon forward query, AllMapping returns the coordinates of all 25 elements in the output matrix by only looking into the matrix metadata.

We define three types of metadata to simplify the interface:
\begin{shortlist}
\item{} oneDMeta: Int
\item{} twoDMeta: (Int, Int)
\item{} threeDMeta: (Int, Int, Int)
\end{shortlist}  
SystemL also uses this data structure metadata to checkout out-of-bound errors if the given key is outside the space of the data structure.

For geometry mapping, the mapping is arbitrary and can not take benefit from data structure metadata, 
Intuitively, users to pass the exact mapping in the form of a collection, inside which is a list of tuples, where each tuple 
contains one set of input element coordinates and one set of output geometry element coordinates. 
This simple solution takes a tremendous amount of space to materialize the mapping in memory and store on disks.
The next section discusses the technical approach to reduce the size of geometry mapping and according indexing strategies.


\subsection{Geometry Mapping}
\label{sec:Design-GeometryMapping}
As can be seen in the geometry mapping example in Figure~\ref{fig:narrowmapping}, the backward query of an output element should return all the input elements in the corresponding geometry. If the output geometry has $N$ elements and the input geometry has $N$ elements, the space complexity of storing such a mapping is $O(N^2)$. 
Inspired by the Scale-invariant feature transform (SIFT) and the SourceExtractor implementation, each of them processes an input matrix and produces thousands of vectors as outputs. An output vector depends on a group of elements in the matrix, and these elements form a two dimensional shape. 
For SIFT, the input elements form a circle with a geographical center and the scale as its radius.
Similarly, the SourceExtractor's input elements form an ellipse. 
SystemL uses a high-order function to describe the geometry such as circle and ellipse. 
Thus the mapping between two geometries is expressed as a tuple of two higher-order functions.
The space complexity goes down to O(1).

Since many softwares such as SIFT and SourceExtractor already return the geographical information with the results.
SystemL exposes a simple interface for two dimensional shapes as special cases.
Users can pass in the geographical information directly to declare rectangle, circle, and ellipse.

Using the high order function involves an issue for query. 
As both the input elements and output elements are encoded with a higher order function. 
And the mapping between input function and output function is stored in a list. 
A query that takes an output element as key thus needs to iterate over all input functions and test if the key is in the function or not.
If the key is in an input function, it then returns the mapping output function, and expands the function to a group of coordinates.
The time complexity of query over geometry mapping is then $O(N^2)$, assuming there are $N$ tuples in the list and each input function expands to $N$ elements.
To speedup the query performance, we implement and compare several indexing strategies, and a detailed discussion is in \S\ref{sec:GeometryIndex}.


\section{Implementations}
\label{sec:Impl}
\zhaonote{talk about how to infer low-high space reduction}
\zhaonote{this section needs rewriting}

This section discusses implementation details for lineage collection, I/O, and geometry mapping index.

\subsection{Metadata Separation}
As discussed in \S\ref{sec:Req-Perf}, writing lineage to persistent storage can introduce a more dramatical slowdown for KeystoneML comparing
to the Hadoop-based systems, since Spark seeks to eliminate unnecessary I/O between memory and persistent storage.
One effective way to reduce the I/O overhead is reduce the lineage data size.

SystemL reduces the lineage data size by 1) separating the metadata from the datasets, 2) tracking only the metadata mapping, 
and 3) taking advantage of the mapping types specified by the user.

In \S\ref{sec:Lin-Pipe-Data}, we say an element in a data structure has two properties: the coordinate and the value.
We refer the coordinate as the metadata of a particular element and the value as the actual data. 
In a backward query of an element throughout the whole pipeline, the query traverses from the last transformer
to the very first transformer in a sequential manner. 
We see that in the last the transformer and other transformers in the middle, the metadata mapping is sufficient to answer
the query of depending input elements of given output elements. Only in the very first transformer do we need both the metadata
to find the metadata of the depending input elements and the actual data of these elements.
So SystemL separates the metadata from actual data and only tracks the metadata mapping.

Another lineage size reduction is contributed by user specified mapping types. 
The {\bf One Mapping} and {\bf All Mapping} are the mapping types behind the OneLineage and AllLineage, respectively.
Both the two mapping types are symmetric.
In {\bf One Mapping} if an output element depends exactly on the input element with the same coordinate, 
the input element value only propagates to that only output element. 
Thus both the forward and backward query return the element that has the identical coordinate as the key.
So is for {\bf All Mapping}, the dependency of all input elements to an output element indicates that every input element's value propagates to all output elements.
Both the forward and backward query shall return all elements in the data structure.
Thus for {\bf One Mapping} and {\bf All Mapping}, recording the metadata of the matrix is enough to answer the forward and backward query.

In contrast, the {\bf Geometry Mapping} and {\bf Sample Mapping} are asymmetric. 
{\bf Sample Mapping} is implemented as a subtype of {\bf Geometry Mapping}, as it samples a random subset of the input data structure and return that subset as output.
We can describe the subset as a list of random geometries (rectangles in column sampling of a matrix), with each input geometry maps to an output geometry.
To record these two mapping types, metadata of data structures is not sufficient to answer the queries. 
SystemL needs to record geometry level mappings instead.

Putting all lineage size reduction strategies together, 
a profile study of the VOCSIFTFisher pipeline shows that the actual data size is 840.9~GB and the metadata size is 61.3~GB. 
Writing only metadata can reduce the total I/O amount by a factor of 13.7x.

For the actual data, SystemL only stores the input dataset for the very beginning transformer as mandatory. 
Users can configure the SystemL to make decisions for other transformers' input and output datasets.




\subsection{Actual Dataset Storage}
In the previous section, we introduce how SystemL separates the metadata from actual data and saves the metadata mapping.
However, SystemL still needs the actual data to answer the forward and backward query.

Considering a multi transformer pipeline, with each transformer having its own input dataset and output dataset.
The minimal actual dataset SystemL needs to save to persistent storage is the input dataset for the very beginning transformer, as
the rest datasets can be reproduced by rerunning the pipeline with the input dataset.

Intuitively, the maximal actual dataset SystemL can save to persistent storage contains all input datasets and output dataset for each transformer.
Since a transformer's output dataset is also the input dataset for the succeeding transformer, thus the maximal dataset without duplicates
should include output datasets of all transformers and the input dataset for the beginning transformer. 

Fundamentally, this is a tradeoff between pipeline slowdown, storage space consumption and query overhead. 
Storing the minimal actual dataset consumes the least space on disk along with the least writing overhead, 
but access to the actual dataset other than the input introduces overhead due to reexecution of the pipeline or a part of the pipeline.
Storing the maximal dataset consumes more disk space and more time, but queries over the actual dataset
have a lower latency.

Assuming SystemL collects lineage information that is complete for the whole pipeline, the space formed by the storage overhead 
(slowdown depends on the storage overhead) and query overhead defines users' flexibility to make actual dataset storing decision.

In other cases, users are only interested in one or few transformers. E.g. debugging the transformer implementation.
He may preempt that transformer's lineage by recording all its input and output dataset. 
And no lineage from other transformers is needed.
SystemL supports such flexibility by letting the user declare and operate lineage for one or few transformers.
In turn, the lineage collected in this case only supports query over each individual transformer.
It these transformers form a sub-pipeline, users can query the lineage throughout this sub-pipeline with SystemL.

As introduced in \S\ref{sec:Design-Lineage}, SystemL collects six types of data for lineage: the input dataset, the output dataset,
the transformer itself, the mapping collection, the model, and the parameter.
Among these six data types, the transformer and the collection of mapping are mandatory. 
The input dataset is mandatory if the transformer is the very first of the pipeline. 
Otherwise, the input dataset is optional.
The output dataset is optional, as it can be reproduced by rerunning the transformer with the input dataset.
The model and parameter are optional.

In terms of data size, the input dataset, the output dataset and the mapping collection is dominant. 
Since all of these three datasets are of the type of RDD abstraction, SystemL calls the RDD primitive to write these datasets
to HDFS, so that each partition is written into the local disk. 
And loading these RDDs from HDFS can benefit from the locality.
For other data types, we encapsulate them into a RDD and use the same approach to write them to HDFS.

All these six datasets are written into a directory that is named by concatenating the transformer label, 
the node sequence id in the pipeline, and the id of the initial input dataset. 
With id of the initial input dataset, SystemL can figure out the branches of the pipeline.
By looking into the input/output RDD id, SystemL is able to tell the sequence of each transformer in the pipeline.
Combining the two previous operations, SystemL can build the lineage of a pipeline in the exact same sequence
as the original pipeline.

\subsection{Geometry Mapping Index}
\label{sec:GeometryIndex}
In \S\ref{sec:Design-GeometryMapping}, we introduced the notion of geometry mapping and how we user higher-order function
to describe the geometry for the purpose of less space consumption. We also present that query of such geometry mapping
can be slow due to the encoding. 
In this section, we discuss four indexing strategies to speedup the query over the geometry mapping. 
The four indexing strategies are referred as {\bf NoIndex}, {\bf Direct}, {\bf RTree}, and {\bf KMeans}, respectively.
For simplicity, we only discuss forward query from the input to the output in this section.
The backward query works in the same way as the forward query.


We use the example in Figure~\ref{fig:example} in the following discussion.
\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/example}
\caption {A geometry mapping example, the left matrix is input and the right matrix is output. The mapping between the geometries are shown using gray shades.
    \label{fig:example}
}
\end{center}
\end{figure}

Table~\ref{tb:example} shows the mapping for each element and the regressed 2D geometry mapping.
\begin{table}[ht]
\begin{center}
    \caption{Element-wise mapping and regressed geometry mapping of the example in Figure~\ref{fig:example}}
    \begin{scriptsize}
    \begin{tabular}{ | p{1.75cm} | p{1.75cm} | p{1.75cm} | p{1.75cm} |}
    \hline
    Input & Geometry & Output & Geometry \\ \hline \hline
    (0,0),...,(1,1) & Circle\_0 & (0,0),(1,0) & Rect\_0 \\ \hline
    (0,2),...,(1,3) & Circle\_1 & (0,1),(1,1) & Rect\_1 \\ \hline
    (2,0),...,(3,1) & Circle\_2 & (0,2),(1,2) & Rect\_2 \\ \hline
    (2,2),...,(3,3) & Circle\_3 & (0,3),(1,3) & Rect\_3 \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:example}
\end{center}   
\end{table} 

\subsubsection{No Index}
Using the example in Table~\ref{tb:example}, with {\bf NoIndex} strategy, query an element needs to traverse all circles (input geometries).
If the element is in a circle, the query returns the all output elements expanded from the mapping rectangle.
The time complexity of a query is $O(N)$, where N is the number of circles. 

\subsubsection{Direct Index}
To build the {\bf Direct} index, we use each input element as key and the associated geometry as the value, as shown in Figure~\ref{fig:direct}.
We rely on the underlying runtime system for the optimization of storing repeated geometries as an single object,
then use an pointer (hash value of the object) to point to the geometry object, as shown in Figure~\ref{fig:direct-optimized}.
This optimization can reduce the storage space of index if the geometry object is larger than the hash value. 
A query of an element takes two steps: the first step returns the hash value for that element and the second step returns the rectangle. 
The space complexity of {\bf Direct} index is $O(k*N)$, where $k$ is the number of circles and $N$ is the number of elements in each circle.
For simplicity, the space complexity of the {\bf Direct} index is $O(N^2)$.
The time complexity of building index and query are $O(N^2)$ and $O(1)$, respectively.

\begin{figure}
\centering
\begin{minipage}{.3\linewidth}
  \includegraphics[width=\linewidth]{pictures/direct}
  \caption{Direct index}
  \label{fig:direct}
\end{minipage}
\hspace{.05\linewidth}
\begin{minipage}{.6\linewidth}
  \includegraphics[width=\linewidth]{pictures/direct-optimized}
  \caption{Direct index with optimization}
  \label{fig:direct-optimized}
\end{minipage}
\end{figure}


\subsubsection{RTree Index}
R-tree~\cite{guttman1984} has been introduced to index multidimensional information.
To simplify the discussion, we use the example shown in Figure~\ref{fig:example} and build the index in a two dimensional space.
The average query time complexity is $O(logN)$, while the worst case is $O(N)$. 
The worst case happens when querying an element in a geomerey where all leaf nodes overlap.
Variants of R-tree, such as R+-tree and R*-tree, seek to improve the worst case query speed by minimizing the overlapped geomerey at leaf nodes.
While they introduce additional overhead when building the index.

In our use cases, it is rare that a single element occurs in all geometries. 
R-tree is sufficient to validate the idea of spatial indexing.
~\zhaonote{mention kd-tree in the related work.}

We use an open source R-tree implementation, Archery~\cite{osheim13} to build the {\bf RTree} index.
In the Archery implementation, both the leaf nodes and the non-leaf nodes are abstracted as rectangles.
To differentiate from the rectangle geometries, we call it box.
To adapt the geometry to Archery, first we compute the bounding box of each circle, then we use
a tuple of the bounding box and the circle as leaf node of the R-tree. 
The index is built upon the bounding boxes. 
The query of an input element (0,0) in the R-tree returns a tuple of the bounding box (center: (0.5,0.5), height:1, width:1) and circle\_0.
Then we further check if the element is in circle\_0. 
The query returns the output elements expanded from Rect\_0 only if the input element is within circle\_0.

The time complexity of building a R-tree is $log(N)$.
Practically the time complexity of query a R-tree is $log(N)$ in our use cases.
And the space complexity for {\bf RTree} index is $log(N)$.

\subsubsection{KMeans Index}
Inspired by the distribution of the geometries in the SIFTExtract and the SourceExtractor transformer,
we come up with a simple index with two level of nodes.
We first cluster the geometries using the KMeans~\cite{macqueen67} algorithm 
and then build a single layer non-leaf nodes with each non-leaf node covering a cluster.

We specify $\sqrt{N}$ clusters, where N is the number of geometries, 
since $\sqrt{N}$ is the optimal cluster number for the worst case query when N geometries are uniformly distributed in a two dimensional space.
Clustering $N$ geometriess into $k$ clusters results in $\frac{N}{k}$ elements in each cluster. 
Thus there are $k$ non-leaf nodes.
A worst case query needs to traverses all $k$ non-leaf nodes and all $\frac{N}{k}$ leaf node in the cluster.
$k+\frac{N}{k}$ has its minimum when $k=\sqrt{N}$. 

The time complexity of building the {\bf KMeans} index is $O(N^{1.5})$.
The average query time complexity is $O(logN)$.
The space complexity of {\bf KMeans} is $O(\sqrt{N})$.

\subsubsection{Index Strategy Comparison}
Table~\ref{tb:index-comparison} summarizes the index building time complexity, 
the query time complexity, and the index space complexity.

\begin{table}[ht]
\begin{center}
    \caption{Index Strategy Comparison}
    \begin{scriptsize}
    \begin{tabular}{ | p{1.75cm} | p{1.75cm} | p{1.75cm} | p{1.75cm} |}
    \hline
    Strategy & Building & Query & Space \\ \hline \hline
    NoIndex & 0 & $O(N)$ & 0 \\ \hline
    Direct & $O(N^2)$ & $O(1)$ & $O(N^2)$ \\ \hline
    RTree & $O(N)$ & $O(logN)$ & $O(logN)$ \\ \hline
    KMeans & $O(N^{1.5})$ & $O(logN)$ & $O(\sqrt{N})$ \\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:index-comparison}
\end{center}   
\end{table}

{\bf Direct}'s query performance is the best from analysis, however the building time overhead and space overhead is quite high.
{\bf RTree} and {\bf KMeans} have a more balanced building time, space and query performance.
In reality, we see that the performance of {\bf RTree} an {\bf KMeans} are significantly affected by parameters such as the
number of boxes in each non-leaf node for {\bf RTree} and the number of iterations for {\bf KMeans}.
A detailed performance study is present in \S\ref{sec:Perf-Index}.

%\begin{table}[ht]
%\begin{center}
%    \caption{Index Strategy Profile on Single SIFTExtracror}
%    \begin{scriptsize}
%    \begin{tabular}{ | p{1.7cm} | p{0.9cm} | p{0.8cm} | p{0.9cm} | p{1cm} | p{1cm} |}
%    \hline
%    Strategy & NoIndex & Direct & RTree & KMeans1 & KMeans5 \\ \hline \hline
%    Average (ms) & 0.28 & 0.04 & 0.095 & 0.073 & 0.049 \\ \hline
%    Stdev & 0.08 & 0.011 & 0.022 & 0.011 & 0.014 \\ \hline
%    Space (MB) & 0.44 & 22.49 & 0.78 & 0.5 & 0.5 \\ \hline
%    \end{tabular}
%    \end{scriptsize}
%    \label{tb:single-SIFT-query}
%\end{center}   
%\end{table}

\section{Evaluation}
\label{sec:Perf}
In this section, we present the performance measurements of SystemL from various aspects.
We demonstrate various built-in indexing strategy for geometry mapping, and profile the tradeoff
between index building time, query performance, memory usage and storage consumption.
We present the end-to-end performance and memory/disk usage for three real ML pipelins.
Then we show the profiling of SystemL's scalability with three real ML pipelines on varying cluster sizes.
Last, we measure the performance of the three use cases presented in \S\ref{sec:Back-cases} with 
fine-grained lineage and its according cost.

Unless otherwise specified, we run all experiments on Amazon EC2 using r3.8xlarge instances.
Each machine has 16 physical cores, 244 GB memory, and two 320GB SSD. 
SystemL is integrated with KeystoneML 0.3 which in turn runs over Spark 1.3.1, HDFS from the
CDH4 distribution of Hadoop. 

\subsection{Geometry Mapping Index}
\label{sec:Perf-Index}
With this experiment we seek to profile the characteristics of the indexing strategies for geometry lineage discussed in \S\ref{sec:GeometrieIndex}.
We use a workload with 5,000 pairs of input and output matrix, with each pair producing 60,000 geometry mappings that are uniformly distributed
in 2D space. The total number of mappings capture is 300 million. The experiment was executed on a 16 node cluster.

Figure~\ref{fig:sift-time} presents the end-to-end performance of this workload with a detailed runtime decomposition. For each data point, 
we run it three times and present the average. 

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/SIFTIndex-Time}
\caption {Runtime profile of the indexing strategies for geometry mapping over 300 million geometry mappings.
    \label{fig:sift-time}
}
\end{center}
\end{figure}

Without lineage tracking, the workload takes 61.3~seconds to finish without lineage tracking, shown as the baseline.
Tracking metadata mapping (geometry mapping) incurs a 89.8~seconds overhead in average. 
Building the index for geometry mapping takes 33.9, 69.9, and 349.0~seconds for RTree, KMeans-1, and KMeans-5.
The Direct strategy crashes due to insufficient memory.

Figure~\ref{fig:sift-query} presents the query performance with these indexing strategies.
For each strategy, we run queries with {1, 100, 1000, 10000} batch size.
NoIndex strategy perform worst among the four strategies, the query latency quickly grow over 10,000~ms with batch size of 100.
The other three strategies show a similar scalability, while KMeans-1 runs 17.9\% and 25.1\% faster than RTree with batch size of 1000 and 10000, respectively.
KMeans-5 runs  31.1\% and 35.2\% faster with batch size of 1000 and 10000, respectively.


\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/SIFTQuery-Time}
\caption {Query performance profile of the geometry mapping with four indexing strategies
    \label{fig:sift-query}
}
\end{center}
\end{figure}

The storage consumption of the four indexing strategies is 30.8GB, 55.4GB, 34.3GB, and 34.3GB 
for NoIndex, RTree, KMeans-1, KMeans-5, respectively. 
For RTree strategy, the index takes as much as 83\% of the mapping size. 
While for RTree, it is 11.4\%.
These four indexing strategies use 71.4\%, 85.7\%, 85.7\%, and 114.3\% more memory comparing to the
baseline case that does not track lineage.

To summarize, NoIndex has the lowest query performance and it is not practical for real use.
KMeans-5 has the best query performance the longest index building time.
RTree has a balanced query performance and building time with the highest storage consumption.
Since storage is not the major bottleneck given the machine capability, we use the RTree strategy
as default, leaving the RTree strategies as users' options.



\subsection{End-to-end Pipeline Performance}
We use three ML pipelines to measure the end-to-end performance and its penalty with lineage capturing
along with resource consumption of memory and storage on a 16 machine cluster.
For each pipeline, we collect both the metadata mapping, the input dataset of the first transformer,  and the output dataset of all transformers.
Table~\ref{tb:apps-overhead} presents the measure numbers, for each data point in the table, it is an
average of three executions. 

We see the performance penalty ranges from 25.6\% to 62.2\%, this performance penalty is comparable to that
of previous works of GMRW (16$\sim$76\%), Newt (20$\sim$50\%), and SubZero(20\%$\sim$150\%).
The memory usage overhead comparing to the baseline case is 200\% for MNIST pipeline, however it is using only
193~GB out of 3.9~TB memory space. 
For more memory-intensive pipelines of SIFTFisher (1.3~TB) and SourceExtractor (1.6~TB), the additional usage is 38.5\% and 43.7\%, respectively.

\begin{table}[ht]
\begin{center}
    \caption{Overhead Profile of SystemL on the SIFTFisher, SourceExtractor, and MNIST Pipelines}
    \begin{scriptsize}
    \begin{tabular}{ | p{1.8cm} | p{1.8cm} | p{1.8cm} | p{1.5cm} | }
    \hline
    Pipeline & SIFTFisher & SourceExtractor & MNIST  \\ \hline \hline
    Baseline & 780.8s & 279.7s & 56.4s \\ \hline
    Perf Penalty & 38.4\% & 25.6\% & 62.2\%   \\ \hline
    Memory & 38.5\% & 43.7\% & 200\%\\ \hline
    Meta Space & 109.9~GB & 204.4~MB & 78.9~MB \\ \hline
    Data Space & 615.7~GB & 506.9~GB & 11.8~GB\\ \hline
    \end{tabular}
    \end{scriptsize}
    \label{tb:apps-overhead}
\end{center}   
\end{table}

\subsection{Scalability}
We use the same three ML pipelines and execute them on clusters with {8, 16, 32, 64} of machines.
We measure the performance of the baseline (without lineage), lineage with only metadata mapping, 
and lineage with both metadata mapping and datasets. 
Then we decompose the running time for NoLineage, Meta, and Data, and show the relative scalability
 to the performance of 8 machines, shown as Figure~\ref{fig:scalability}.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/Scalability}
\caption {Relative Speedup of the VOCSIFTFisher, Source Extractor, and MNIST Pipelines with SystemL over Eight Nodes.
    \label{fig:scalability}
}
\end{center}
\end{figure}

One pattern we can conclude from the scalability measurements is that SystemL scales no worse than the ML pipelines.
This is due to the fact that SystemL captures and stores the lineage information via the RDD abstraction, which perform
in a scalable manner. 
The metadata mapping capturing is more likely to be impacted by the ML pipeline comparing to the case where both metadata
mapping and datasets are captured, since it is a relatively less data-intensive operations.
Combining pipeline stats shown in Table~\ref{tb:apps-overhead}, SystemL scales  well when collecting larger datasets.
In the SIFTFisher case, capturing both metadata mapping and datasets shows scalability that is higher than ideal.
This is because the SIFTFisher performance is memory-bounded on 8 machines, once the memory bound is relieved, the actual performance
exceeds the expectation.

\subsection{Query Performance}
\begin{figure}[h]
\begin{center}
    \includegraphics[width=85mm]{pictures/TypeQuery-Time}
\caption {Query time for the seven lineage types (in log scale).
    \label{fig:typequery}
}
\end{center}
\end{figure}

\subsection{Use Case}
In the {\bf code validation} case, users instrument the SIFTFisher pipeline by only recording the metadata mapping of the SIFTExtractor transformer.
Upon validation, users can query extracted feature, and locate the positions of the feature in the input image. With some external visualization tool,
users can visualize corresponding pixels in the original image. Such an instrumentation introduces a performance penalty of 15.8\% on a 16 machine 
cluster.
Query a single feature takes $0.14\pm0.0062$~seconds while a batch query of 1,000 features takes $0.33\pm0.001$~seconds.

In the {\bf results inspection} use case, users of the SourceExtractor pipeline collects the metadata mapping in all transformers along with the output 
datasets of the last step. By joining the metadata mapping with output datasets, which contains the luminosity information, users can first filter out the
objects with a threshold brightness. Then query the metadata mapping to find the positions of corresponding pixels in the input images.
This instrumentation introduces a performance penalty of 40.8\% on a 8 machine cluster. 
Filtering luminous flux $>300,000$~lumen over 1 million objects returns 10,283 objects
with its corresponding pixel positions in $1.84\pm0.14$~seconds.

In the {\bf data cleaning} use case, the user of the SIFTFisher pipeline want to remove the training images that has both the human and table as
he sees such a combination results in lower detection accuracy for the human category .
So he instruments pipeline collecting all metadata mappings and the output datasets for NormalizeRows transformer. 
Upon removal, he first joins the output dataset of NormalizeRows (a RDD of vectors) transformer with the labels.
Then he filters out the corresponding vectors with both human and table in it. 
From here, he can try retrain the model with the cleaned dataset. 
This particular instrumentation introduces a performance penalty of 46.8\%, and the removal takes $25.7\pm0.27$ seconds.
Comparing to the case of rerunning the data preparation phase of the pipeline, using lineage information speeds up the turnaround
time by 16.4x. Further, to confirm the correctness of the removal, the users can query the lineage backward to find corresponding 
images and conduct a verification.

\section{Related Work}
\label{sec:Related}
Researchers have extensively studied data lineage (in some cases referred as data provenance) for various purposes.
For scientific workflow systems such as  Chimera~\cite{foster02}, Taverna~\cite{oinn02}, and ESSW~\cite{frew01}, 
collects coarse-grained lineage of file metadata and according computation. 
Similar work has been well summarized by survey~\cite{simmhan05, freire08, bose05}
Recent work~\cite{altintas10} investigates how to integrate the lineage of workflow executions
in a collaborative environment. Coarse-grained lineage is useful in these systems to trace the data and code dependencies.
Since the lineage collected is coarse-grained, it is not sufficient for cell level diagnosis of ML pipelines.

Fine-grained lineage has been investigated in the context of data visualization~\cite{stonebraker93, woodruff97},  
data warehouse~\cite{cui00, cui03}, RDBMS~\cite{widom04}, and user-curated database~\cite{buneman06}.
Recent works of GMRW~\cite{ikeda11} and Newt~\cite{logothetis13} collects fine-grained lineage for the mapreduce
systems. SubZero~\cite{wu13} proposes a prototype lineage system for array-based database system SciDB~\cite{brown10}.
All these systems employ a bottom-up approach to design lineage capturing interface as the underlying systems all have well regulated low level operators.
For example, the data warehouse has the aggregate, select, project, join operators. 
RDMBS has its SQL operators. Mapreduce systems formulates the computation as mappers and reducers.
The curated database defines four operators of insert, delete, copy and paste.
One of SubZero's contributions is the region lineage that collects fine-grained lineage for UDFs besides the built-in operators in SciDB. 
In contrast, an ML pipeline can be viewed as a chain of user-defined data transformations, and the users often have the knowledge 
of lineage types and data of interest since they are the designers of the pipeline. 
Though lineage capturing interface designed with the bottom-up approach is general to support applications running on top these systems,
it exposes very limited flexibility of lineage type declaration and instrumentation to users.

Researchers also have tried to collect fine-grained lineage with other approaches.
Weak inversion and verification methods~\cite{woodruff97} are proposed to support approximate lineage collection.
Dynamic program analysis is employed to capture fine-grained lineage inside non-relational operators~\cite{zhang07} with 
a 7.5$\sim$39.8x slowdown comparing the original program. 
These approaches do not fit the ML pipeline lineage scenario since they either loose the lineage accuracy or introduce
excessive performance penalty.
The Arnold system~\cite{devecsery14} captures fine-grained lineage at operating system process level by recoding
all non-deterministic data such as the order, return values, modified memory addresses, and many others. 
This approach can collect fine-grain lineage for any process running on top of the operating system, however
it looses track of the upper level data structures (e.g., vector, matrix, images), which are fundamental to ML pipelines.
One step beyond fine-grained lineage, researches~\cite{meliou10, meliou11} explores the causality and responsibility
of inputs to the results. Though the causality and responsibility is relatively easier for data transformations in ML pipelines.
It can be applied in the training process analysis to infer the contribution of individual training sample to the model and predictions.


\section{Conclusion and Future Work}
\label{sec:Conclusion}
To enable accurate, flexible, low-overhead lineage collection and responsive lineage query, we present the SystemL lineage system.
SystemL exposes a concise yet powerful lineage capturing interface for users by formulating the ML pipeline lineage as a sequence
of multi-dimensional space transformations. The proposed interface has a coverage of 87\% of the transformers and estimators
in the current KeystoneML code base. Combining the metadata of data structures, the high order function approach and spatial indexing
strategy, SystemL is able to answer typical query within $\sim$100~ms and $\sim$1~second. Practically, SystemL introduces a
performance penalty of 16\%-54\% depending on the pipeline and use case.

As future work, we are investigating the diagnosability of the training process in the estimator. We seek to provide the users with
the fine-grained lineage information inside an estimator to answer queries of supporting training samples for
a prediction and the impact of certain feature removal or training data removal.

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP,  The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple, Inc., Blue Goji, Bosch, C3Energy, Cisco, Cray, Cloudera, EMC, Ericsson, Facebook, Guavus, Huawei, Intel, Microsoft, NetApp, Pivotal, Samsung, Splunk, Virdata, VMware, and Yahoo!. 

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{Lineage} % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns



\balancecolumns

% That's all folks!
\end{document}
